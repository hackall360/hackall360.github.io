<!DOCTYPE html><html lang="en" class="dark"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>SOLLOL | hackall360 Projects</title><meta name="description" content="Performance-aware routing for distributed Ollama deployments with Ray and Dask."><meta property="og:type" content="website"><meta property="og:site_name" content="hackall360"><meta property="og:title" content="SOLLOL | hackall360 Projects"><meta property="og:description" content="Performance-aware routing for distributed Ollama deployments with Ray and Dask."><meta property="og:image" content="/social-share.jpg"><meta property="og:url" content="https://hackall360.github.io/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="SOLLOL | hackall360 Projects"><meta name="twitter:description" content="Performance-aware routing for distributed Ollama deployments with Ray and Dask."><meta name="twitter:image" content="/social-share.jpg"><link rel="icon" href="/favicon.svg" type="image/svg+xml"><link rel="alternate icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><meta name="theme-color" content="#0f172a" media="(prefers-color-scheme: dark)"><meta name="theme-color" content="#f1f5f9" media="(prefers-color-scheme: light)"><script type="application/ld+json">
        {JSON.stringify(structuredData)}
      </script><link rel="stylesheet" href="/_astro/about.DGIKsCa0.css"></head> <body class="bg-transparent text-inherit selection:bg-accent/30 selection:text-accent-light"> <div class="relative z-10 flex min-h-screen flex-col"> <header class="relative border-b border-accent/10 bg-surface/80 shadow-lg shadow-accent/10 backdrop-blur"> <div class="absolute inset-x-0 top-0 h-px bg-gradient-to-r from-transparent via-accent/60 to-transparent" aria-hidden="true"></div> <div class="mx-auto flex max-w-5xl flex-col gap-4 px-6 py-6 sm:flex-row sm:items-center sm:justify-between"> <div> <p class="font-mono text-xs uppercase tracking-[0.3em] text-accent/80">$ whoami</p> <h1 class="mt-1 font-mono text-xl font-semibold text-white"> <span class="text-accent-light">hack</span>all360
</h1> </div> <nav class="flex flex-wrap items-center gap-3 font-mono text-sm text-neutral-soft" aria-label="Primary navigation"> <a class="rounded-md border border-accent/15 px-3 py-1 transition-colors duration-200 ease-spring-out focus-visible:outline-none hover:border-accent hover:bg-accent/10 hover:text-accent-light" href="/"> <span>Home</span>  </a><a class="rounded-md border border-accent/15 px-3 py-1 transition-colors duration-200 ease-spring-out focus-visible:outline-none hover:border-accent hover:bg-accent/10 hover:text-accent-light" href="/projects"> <span>Projects</span>  </a><a class="rounded-md border border-accent/15 px-3 py-1 transition-colors duration-200 ease-spring-out focus-visible:outline-none hover:border-accent hover:bg-accent/10 hover:text-accent-light" href="/about"> <span>About</span>  </a><a class="rounded-md border border-accent/15 px-3 py-1 transition-colors duration-200 ease-spring-out focus-visible:outline-none hover:border-accent hover:bg-accent/10 hover:text-accent-light" href="/now"> <span>Now</span>  </a><a class="rounded-md border border-accent/15 px-3 py-1 transition-colors duration-200 ease-spring-out focus-visible:outline-none hover:border-accent hover:bg-accent/10 hover:text-accent-light" href="/notes"> <span>Notes</span>  </a><a class="rounded-md border border-accent/15 px-3 py-1 transition-colors duration-200 ease-spring-out focus-visible:outline-none hover:border-accent hover:bg-accent/10 hover:text-accent-light" href="/assistant"> <span>Assistant</span>  </a> </nav> </div> </header> <main class="flex-1">  <article class="relative mx-auto flex w-full max-w-5xl flex-col gap-12 px-6 py-16"> <nav aria-label="Breadcrumb" class="text-sm text-slate-400"> <ol class="flex flex-wrap items-center gap-2 text-xs uppercase tracking-[0.25em]"> <li class="flex items-center gap-2">  <a class="text-accent-light hover:text-accent" href="/">Home</a> </li><li class="flex items-center gap-2"> <span class="text-slate-600">/</span> <a class="text-accent-light hover:text-accent" href="/projects">Projects</a> </li><li class="flex items-center gap-2" aria-current="page"> <span class="text-slate-600">/</span> <span class="text-slate-300">SOLLOL</span> </li> </ol> </nav> <header class="space-y-6"> <p class="text-sm font-semibold uppercase tracking-[0.35em] text-accent-light">Project Case Study</p> <h1 class="text-4xl font-bold text-slate-100 sm:text-5xl">SOLLOL</h1> <p class="max-w-3xl text-lg text-slate-300">Performance-aware routing for distributed Ollama deployments with Ray and Dask.</p> <div class="flex flex-col gap-3 sm:flex-row sm:items-center"> <a href="/projects" class="inline-flex items-center justify-center rounded-full border border-slate-700/70 px-5 py-2 text-sm font-medium uppercase tracking-wide text-slate-200 transition hover:border-accent hover:text-accent">
â† Back to Projects
</a> <a href="https://github.com/hackall360/SOLLOL" target="_blank" rel="noreferrer" class="inline-flex items-center justify-center rounded-full border border-accent/70 bg-accent/10 px-5 py-2 text-sm font-semibold uppercase tracking-wide text-accent-light transition hover:bg-accent/20 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-accent focus-visible:ring-offset-2 focus-visible:ring-offset-slate-900">
View on GitHub
</a>  </div> </header> <section class="grid gap-10 rounded-3xl border border-slate-800/80 bg-slate-900/50 p-8 shadow-xl shadow-accent/5 sm:grid-cols-2"> <div class="space-y-6"> <div> <h2 class="text-xs font-semibold uppercase tracking-[0.35em] text-accent-light">Problem</h2> <p class="mt-3 text-base leading-relaxed text-slate-300">How do we push this build forward without compromising speed, reliability, or security?</p> </div> <div> <h2 class="text-xs font-semibold uppercase tracking-[0.35em] text-accent-light">Solution</h2> <p class="mt-3 text-base leading-relaxed text-slate-300">Lean on tight feedback loops, automation, and thoughtful defaults. The README below unpacks the current implementation.</p> </div> </div> <div class="space-y-6"> <div> <h2 class="text-xs font-semibold uppercase tracking-[0.35em] text-accent-light">Tech Stack</h2> <ul class="mt-3 flex flex-wrap gap-2"> <li class="rounded-full border border-accent/40 bg-accent/10 px-3 py-1 text-xs font-semibold uppercase tracking-wide text-accent-light"> AI </li><li class="rounded-full border border-accent/40 bg-accent/10 px-3 py-1 text-xs font-semibold uppercase tracking-wide text-accent-light"> Utility </li> </ul> </div> <div> <h2 class="text-xs font-semibold uppercase tracking-[0.35em] text-accent-light">Results</h2> <ul class="mt-3 space-y-2 text-base leading-relaxed text-slate-300"> <li class="flex items-start gap-2"> <span class="mt-2 h-1.5 w-1.5 rounded-full bg-accent"></span> <span>Track progress through commit history and release notes in the repository.</span> </li> </ul> </div> </div> </section> <section class="space-y-6"> <div class="flex flex-col gap-2"> <h2 class="text-sm font-semibold uppercase tracking-[0.35em] text-accent-light">Deep Dive</h2> <p class="text-base text-slate-400">
Detailed notes that surface the engineering trade-offs, safeguards, and iteration loops powering this build.
</p> </div> <div class="prose prose-invert max-w-none prose-headings:font-semibold prose-headings:text-slate-100 prose-a:text-accent-light prose-code:bg-surface-elevated/60 prose-code:text-accent-light prose-pre:bg-slate-900/70"> <div><h1 id="sollol---production-ready-orchestration-for-local-llm-clusters">SOLLOL - Production-Ready Orchestration for Local LLM Clusters</h1>
<div align="center">
<p><a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python 3.8+"></a>
<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT"></a>
<a href="https://github.com/BenevolentJoker-JohnL/SOLLOL/actions/workflows/tests.yml"><img src="https://github.com/BenevolentJoker-JohnL/SOLLOL/actions/workflows/tests.yml/badge.svg" alt="Tests"></a>
<a href="https://codecov.io/gh/BenevolentJoker-JohnL/SOLLOL"><img src="https://codecov.io/gh/BenevolentJoker-JohnL/SOLLOL/branch/main/graph/badge.svg" alt="codecov"></a>
<a href="https://ollama.ai/"><img src="https://img.shields.io/badge/Ollama-Compatible-green.svg" alt="Ollama"></a>
<a href="https://github.com/ggerganov/llama.cpp"><img src="https://img.shields.io/badge/llama.cpp-Integrated-orange.svg" alt="llama.cpp"></a></p>
<p><strong>Open-source orchestration layer that combines intelligent task routing with distributed model inference for local LLM clusters.</strong></p>
<p><a href="#quick-start">Quick Start</a> â€¢ <a href="#why-sollol">Features</a> â€¢ <a href="#architecture">Architecture</a> â€¢ <a href="#documentation">Documentation</a> â€¢ <a href="#examples">Examples</a></p>
</div>
<hr>
<h2 id="-what-is-sollol">ğŸ¯ What is SOLLOL?</h2>
<p>SOLLOL (Super Ollama Load balancer &#x26; Orchestration Layer) transforms your collection of Ollama nodes into an <strong>intelligent AI cluster</strong> with adaptive routing and automatic failoverâ€”all running on your own hardware.</p>
<h3 id="the-problem">The Problem</h3>
<p>You have multiple machines with GPUs running Ollama, but:</p>
<ul>
<li>âŒ Manual node selection for each request</li>
<li>âŒ No way to run models larger than your biggest GPU</li>
<li>âŒ Canâ€™t distribute multi-agent workloads efficiently</li>
<li>âŒ No automatic failover or load balancing</li>
<li>âŒ Zero visibility into cluster performance</li>
</ul>
<h3 id="the-sollol-solution">The SOLLOL Solution</h3>
<p>SOLLOL provides:</p>
<ul>
<li>âœ… <strong>Intelligent routing</strong> that learns which nodes work best for each task</li>
<li>âœ… <strong>Model sharding</strong> to run 70B+ models across multiple machines</li>
<li>âœ… <strong>Parallel agent execution</strong> for multi-agent frameworks</li>
<li>âœ… <strong>Auto-discovery</strong> of all nodes and capabilities</li>
<li>âœ… <strong>Built-in observability</strong> with real-time metrics</li>
<li>âœ… <strong>Zero-config deployment</strong> - just point and go</li>
</ul>
<hr>
<h2 id="-quickstart-3-commands">âš¡ Quickstart (3 Commands)</h2>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># 1. Install SOLLOL</span></span>
<span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> sollol</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. Start the dashboard (optional but recommended)</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.dashboard_service</span><span style="color:#E1E4E8"> &#x26;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 3. Run your first query</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -c</span><span style="color:#9ECBFF"> "from sollol import OllamaPool; pool = OllamaPool.auto_configure(); print(pool.chat(model='llama3.2', messages=[{'role': 'user', 'content': 'Hello!'}])['message']['content'])"</span></span>
<span class="line"></span></code></pre>
<p><strong>What just happened?</strong></p>
<ul>
<li>âœ… SOLLOL auto-discovered all Ollama nodes on your network</li>
<li>âœ… Intelligently routed your request to the best available node</li>
<li>âœ… Dashboard live at <code>http://localhost:8080</code> (shows routing decisions, metrics, logs)</li>
</ul>
<p><strong>Expected output:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Discovering Ollama nodes...</span></span>
<span class="line"><span>Found 3 nodes: 10.9.66.45:11434, 10.9.66.154:11434, localhost:11434</span></span>
<span class="line"><span>Selected node: 10.9.66.45:11434 (GPU, 12ms latency)</span></span>
<span class="line"><span>Hello! How can I help you today?</span></span>
<span class="line"><span></span></span></code></pre>
<p><strong>Next steps:</strong></p>
<ul>
<li>Visit <code>http://localhost:8080</code> to see the dashboard</li>
<li>Check <a href="#full-quick-start">Full Quick Start</a> for production setup</li>
<li>Read <a href="#examples">Examples</a> for multi-agent, batch, and sharding patterns</li>
</ul>
<hr>
<h2 id="-full-quick-start">ğŸš€ Full Quick Start</h2>
<h3 id="installation">Installation</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> sollol</span></span>
<span class="line"></span></code></pre>
<h3 id="basic-usage">Basic Usage</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Auto-discover nodes and start routing</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Make requests - SOLLOL routes intelligently</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Hello!"</span><span style="color:#E1E4E8">}]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h3 id="enable-real-time-gpu-monitoring">Enable Real-Time GPU Monitoring</h3>
<p>For accurate VRAM-aware routing, install the GPU reporter on each node:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># On each Ollama node, run:</span></span>
<span class="line"><span style="color:#B392F0">sollol</span><span style="color:#9ECBFF"> install-gpu-reporter</span><span style="color:#79B8FF"> --redis-host</span><span style="color:#F97583"> &#x3C;</span><span style="color:#9ECBFF">redis-server-i</span><span style="color:#E1E4E8">p</span><span style="color:#F97583">></span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Example:</span></span>
<span class="line"><span style="color:#B392F0">sollol</span><span style="color:#9ECBFF"> install-gpu-reporter</span><span style="color:#79B8FF"> --redis-host</span><span style="color:#79B8FF"> 10.9.66.154</span></span>
<span class="line"></span></code></pre>
<p><strong>What this does:</strong></p>
<ul>
<li>Installs vendor-agnostic GPU monitoring (NVIDIA/AMD/Intel via <code>gpustat</code>)</li>
<li>Publishes real-time VRAM stats to Redis every 5 seconds</li>
<li>SOLLOL uses this data for intelligent routing decisions</li>
<li>See <a href="GPU_MONITORING_GUIDE.md">GPU Monitoring Guide</a> for details</li>
</ul>
<p><strong>Without GPU monitoring:</strong> SOLLOL falls back to estimates which may be inaccurate.</p>
<hr>
<h2 id="-screenshots">ğŸ“¸ Screenshots</h2>
<h3 id="dashboard-overview">Dashboard Overview</h3>
<p><img src="docs/screenshots/dashboard-overview.png" alt="SOLLOL Unified Dashboard">
<em>Real-time monitoring with P50/P95/P99 latency metrics, network nodes, RPC backends, and active applications</em></p>
<h3 id="ray--dask-integration">Ray &#x26; Dask Integration</h3>
<p><img src="docs/screenshots/dashboard-metrics.png" alt="Ray and Dask Dashboards">
<em>Embedded Ray and Dask dashboards for distributed task monitoring</em></p>
<h3 id="activity-monitoring">Activity Monitoring</h3>
<p><img src="docs/screenshots/dashboard-activity.png" alt="Real-time Activity Logs">
<em>Live request/response activity streams from Ollama nodes and RPC backends</em></p>
<h3 id="applications--traces">Applications &#x26; Traces</h3>
<p><img src="docs/screenshots/dashboard-ray-dask.png" alt="Performance Analytics">
<em>Applications, distributed traces, and Ollama activity logs with real-time request/response tracking</em></p>
<hr>
<h2 id="-why-sollol">ğŸ”¥ Why SOLLOL?</h2>
<h3 id="1-two-distribution-modes-in-one-system">1. <strong>Two Distribution Modes in One System</strong></h3>
<p>SOLLOL combines both task distribution and model sharding:</p>
<h4 id="-task-distribution-horizontal-scaling">ğŸ“Š Task Distribution (Horizontal Scaling)</h4>
<p>Distribute <strong>multiple requests</strong> across your cluster in parallel:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Run 10 agents simultaneously across 5 nodes</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"><span style="color:#E1E4E8">responses </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> asyncio.gather(</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">[</span></span>
<span class="line"><span style="color:#E1E4E8">    pool.chat(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> _ </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#6A737D"># Parallel execution across available nodes</span></span>
<span class="line"></span></code></pre>
<h4 id="-model-sharding-vertical-scaling">ğŸ§© Model Sharding (Vertical Scaling)</h4>
<p>Run <strong>single large models</strong> that donâ€™t fit on one machine:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Run larger models across multiple nodes</span></span>
<span class="line"><span style="color:#6A737D"># Note: Verified with 13B across 2-3 nodes; larger models not extensively tested</span></span>
<span class="line"><span style="color:#E1E4E8">router </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> HybridRouter(</span></span>
<span class="line"><span style="color:#FFAB70">    enable_distributed</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    num_rpc_backends</span><span style="color:#F97583">=</span><span style="color:#79B8FF">4</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> router.route_request(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3:70b"</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Sharded automatically</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>Use them together!</strong> Small models use task distribution, large models use sharding.</p>
<hr>
<h3 id="2-intelligent-not-just-balanced">2. <strong>Intelligent, Not Just Balanced</strong></h3>
<p>SOLLOL doesnâ€™t just distribute requests randomlyâ€”it <strong>learns</strong> and <strong>optimizes</strong>:</p>








































<table><thead><tr><th>Feature</th><th>Simple Load Balancer</th><th>SOLLOL</th></tr></thead><tbody><tr><td><strong>Routing</strong></td><td>Round-robin</td><td>Context-aware scoring</td></tr><tr><td><strong>Learning</strong></td><td>None</td><td>Adapts from performance history</td></tr><tr><td><strong>Resource Awareness</strong></td><td>None</td><td>GPU/CPU/memory-aware</td></tr><tr><td><strong>Task Optimization</strong></td><td>None</td><td>Routes by task type complexity</td></tr><tr><td><strong>Failover</strong></td><td>Manual</td><td>Automatic with health checks</td></tr><tr><td><strong>Priority</strong></td><td>FIFO</td><td>Priority queue with fairness</td></tr></tbody></table>
<p><strong>Example</strong>: SOLLOL automatically routes:</p>
<ul>
<li>Heavy generation tasks â†’ GPU nodes with 24GB VRAM</li>
<li>Fast embeddings â†’ CPU nodes or smaller GPUs</li>
<li>Critical requests â†’ Fastest, most reliable nodes</li>
<li>Batch processing â†’ Lower priority, distributed load</li>
</ul>
<hr>
<h3 id="3-production-ready-from-day-one">3. <strong>Production-Ready from Day One</strong></h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#79B8FF"> SOLLOL</span><span style="color:#E1E4E8">, SOLLOLConfig</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Literally 3 lines to production</span></span>
<span class="line"><span style="color:#E1E4E8">config </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SOLLOLConfig.auto_discover()</span></span>
<span class="line"><span style="color:#E1E4E8">sollol </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SOLLOL(config)</span></span>
<span class="line"><span style="color:#E1E4E8">sollol.start()  </span><span style="color:#6A737D"># âœ… Gateway running on :8000</span></span>
<span class="line"></span></code></pre>
<p><strong>Out of the box</strong>:</p>
<ul>
<li>Auto-discovery of Ollama nodes</li>
<li>Health monitoring and failover</li>
<li>Prometheus metrics</li>
<li>Web dashboard</li>
<li>Connection pooling</li>
<li>Request hedging</li>
<li>Priority queuing</li>
</ul>
<hr>
<h3 id="4-unified-observability-for-your-entire-ai-network">4. <strong>Unified Observability for Your Entire AI Network</strong></h3>
<p>SOLLOL provides a <strong>single pane of glass</strong> to monitor every application and every node in your distributed AI network.</p>
<ul>
<li>âœ… <strong>Centralized Dashboard</strong>: One web interface shows all applications, nodes, and RPC backends.</li>
<li>âœ… <strong>Multi-App Tracking</strong>: See which applications (e.g., SynapticLlamas, custom agents) are using the cluster in real-time.</li>
<li>âœ… <strong>Network-Wide Visibility</strong>: The dashboard runs as a persistent service, discovering and monitoring all components even if no applications are running.</li>
<li>âœ… <strong>Zero-Config</strong>: Applications automatically appear in the dashboard with no extra code required.</li>
</ul>
<p>This moves beyond per-application monitoring to provide true, centralized observability for your entire infrastructure.</p>
<hr>
<h2 id="ï¸-architecture">ğŸ—ï¸ Architecture</h2>
<h3 id="high-level-overview">High-Level Overview</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚                  Your Application                       â”‚</span></span>
<span class="line"><span>â”‚         (SynapticLlamas, custom agents, etc.)          â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>                       â”‚</span></span>
<span class="line"><span>                       â–¼</span></span>
<span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚                 SOLLOL Gateway (:8000)                  â”‚</span></span>
<span class="line"><span>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚</span></span>
<span class="line"><span>â”‚  â”‚         Intelligent Routing Engine               â”‚  â”‚</span></span>
<span class="line"><span>â”‚  â”‚  â€¢ Analyzes: task type, complexity, resources    â”‚  â”‚</span></span>
<span class="line"><span>â”‚  â”‚  â€¢ Scores: all nodes based on context            â”‚  â”‚</span></span>
<span class="line"><span>â”‚  â”‚  â€¢ Learns: from performance history              â”‚  â”‚</span></span>
<span class="line"><span>â”‚  â”‚  â€¢ Routes: to optimal node                       â”‚  â”‚</span></span>
<span class="line"><span>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚</span></span>
<span class="line"><span>â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚</span></span>
<span class="line"><span>â”‚  â”‚          Priority Queue + Failover               â”‚  â”‚</span></span>
<span class="line"><span>â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>         â”‚                         â”‚</span></span>
<span class="line"><span>         â–¼                         â–¼</span></span>
<span class="line"><span>  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>  â”‚ Task Mode   â”‚          â”‚  Shard Mode  â”‚</span></span>
<span class="line"><span>  â”‚ Ray Cluster â”‚          â”‚  llama.cpp   â”‚</span></span>
<span class="line"><span>  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>         â”‚                         â”‚</span></span>
<span class="line"><span>         â–¼                         â–¼</span></span>
<span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚              Your Heterogeneous Cluster                 â”‚</span></span>
<span class="line"><span>â”‚  GPU (24GB) â”‚ GPU (16GB) â”‚ CPU (64c) â”‚ GPU (8GB) â”‚...  â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span></span></span></code></pre>
<h3 id="how-routing-works">How Routing Works</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># 1. Request arrives</span></span>
<span class="line"><span style="color:#79B8FF">POST</span><span style="color:#F97583"> /</span><span style="color:#E1E4E8">api</span><span style="color:#F97583">/</span><span style="color:#E1E4E8">chat {</span></span>
<span class="line"><span style="color:#9ECBFF">  "model"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">  "messages"</span><span style="color:#E1E4E8">: [{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Complex analysis task..."</span><span style="color:#E1E4E8">}],</span></span>
<span class="line"><span style="color:#9ECBFF">  "priority"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">8</span></span>
<span class="line"><span style="color:#E1E4E8">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. SOLLOL analyzes</span></span>
<span class="line"><span style="color:#E1E4E8">task_type </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "generation"</span><span style="color:#6A737D">       # Auto-detected</span></span>
<span class="line"><span style="color:#E1E4E8">complexity </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "high"</span><span style="color:#6A737D">             # Token count analysis</span></span>
<span class="line"><span style="color:#E1E4E8">requires_gpu </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> True</span><span style="color:#6A737D">             # Based on task</span></span>
<span class="line"><span style="color:#E1E4E8">estimated_duration </span><span style="color:#F97583">=</span><span style="color:#79B8FF"> 3.</span><span style="color:#FDAEB7;font-style:italic">2s</span><span style="color:#6A737D">       # From history</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 3. SOLLOL scores all nodes</span></span>
<span class="line"><span style="color:#E1E4E8">Node A (</span><span style="color:#79B8FF">GPU</span><span style="color:#FDAEB7;font-style:italic"> 24GB</span><span style="color:#E1E4E8">, load: </span><span style="color:#79B8FF">0.2</span><span style="color:#E1E4E8">, latency: </span><span style="color:#FDAEB7;font-style:italic">120ms</span><span style="color:#E1E4E8">) â†’ Score: </span><span style="color:#79B8FF">185.3</span><span style="color:#E1E4E8"> âœ“ </span><span style="color:#79B8FF">WINNER</span></span>
<span class="line"><span style="color:#E1E4E8">Node B (</span><span style="color:#79B8FF">GPU</span><span style="color:#FDAEB7;font-style:italic"> 8GB</span><span style="color:#E1E4E8">,  load: </span><span style="color:#79B8FF">0.6</span><span style="color:#E1E4E8">, latency: </span><span style="color:#FDAEB7;font-style:italic">200ms</span><span style="color:#E1E4E8">) â†’ Score: </span><span style="color:#79B8FF">92.1</span></span>
<span class="line"><span style="color:#E1E4E8">Node C (</span><span style="color:#79B8FF">CPU</span><span style="color:#E1E4E8"> only, load: </span><span style="color:#79B8FF">0.1</span><span style="color:#E1E4E8">, latency: </span><span style="color:#FDAEB7;font-style:italic">80ms</span><span style="color:#E1E4E8">)  â†’ Score: </span><span style="color:#79B8FF">41.2</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 4. Routes to Node A, monitors execution, learns for next time</span></span>
<span class="line"></span></code></pre>
<p><strong>Scoring Algorithm</strong>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>Score = 100.0 (baseline)</span></span>
<span class="line"><span>      Ã— success_rate (0.0-1.0)</span></span>
<span class="line"><span>      Ã· (1 + latency_penalty)</span></span>
<span class="line"><span>      Ã— gpu_bonus (1.5x if GPU available &#x26; needed)</span></span>
<span class="line"><span>      Ã· (1 + load_penalty)</span></span>
<span class="line"><span>      Ã— priority_alignment</span></span>
<span class="line"><span>      Ã— task_specialization</span></span>
<span class="line"><span></span></span></code></pre>
<hr>
<h2 id="-installation">ğŸ“¦ Installation</h2>
<h3 id="quick-install-pypi">Quick Install (PyPI)</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> sollol</span></span>
<span class="line"></span></code></pre>
<h3 id="from-source">From Source</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">git</span><span style="color:#9ECBFF"> clone</span><span style="color:#9ECBFF"> https://github.com/BenevolentJoker-JohnL/SOLLOL.git</span></span>
<span class="line"><span style="color:#79B8FF">cd</span><span style="color:#9ECBFF"> SOLLOL</span></span>
<span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#79B8FF"> -e</span><span style="color:#9ECBFF"> .</span></span>
<span class="line"></span></code></pre>
<hr>
<h2 id="-quick-start">âš¡ Quick Start</h2>
<h3 id="1-synchronous-api-no-asyncawait-needed">1. Synchronous API (No async/await needed!)</h3>
<p><strong>New in v0.3.6:</strong> SOLLOL now provides a synchronous API for easier integration with non-async applications.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.sync_wrapper </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.priority_helpers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> Priority</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Auto-discover and connect to all Ollama nodes</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Make requests - SOLLOL routes intelligently</span></span>
<span class="line"><span style="color:#6A737D"># No async/await needed!</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Hello!"</span><span style="color:#E1E4E8">}],</span></span>
<span class="line"><span style="color:#FFAB70">    priority</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">Priority.</span><span style="color:#79B8FF">HIGH</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Semantic priority levels</span></span>
<span class="line"><span style="color:#FFAB70">    timeout</span><span style="color:#F97583">=</span><span style="color:#79B8FF">60</span><span style="color:#6A737D">  # Request timeout in seconds</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response[</span><span style="color:#9ECBFF">'message'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'content'</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Routed to: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">response.get(</span><span style="color:#9ECBFF">'_sollol_routing'</span><span style="color:#E1E4E8">, {}).get(</span><span style="color:#9ECBFF">'host'</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">'unknown'</span><span style="color:#E1E4E8">)</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>Key features of synchronous API:</strong></p>
<ul>
<li>âœ… No async/await syntax required</li>
<li>âœ… Works with synchronous agent frameworks</li>
<li>âœ… Same intelligent routing and features</li>
<li>âœ… Runs async code in background thread automatically</li>
</ul>
<hr>
<h3 id="2-async-api-original">2. Async API (Original)</h3>
<p>For async applications, use the original async API:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Auto-discover and connect to all Ollama nodes</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Make requests - SOLLOL routes intelligently</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Hello!"</span><span style="color:#E1E4E8">}]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response[</span><span style="color:#9ECBFF">'message'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'content'</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Routed to: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">response[</span><span style="color:#9ECBFF">'_sollol_routing'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'host'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Task type: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">response[</span><span style="color:#9ECBFF">'_sollol_routing'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'task_type'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<hr>
<h3 id="3-priority-based-multi-agent-execution">3. Priority-Based Multi-Agent Execution</h3>
<p><strong>New in v0.3.6:</strong> Use semantic priority levels and role-based mapping.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.sync_wrapper </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.priority_helpers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> Priority, get_priority_for_role</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Define agents with different priorities</span></span>
<span class="line"><span style="color:#E1E4E8">agents </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> [</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Researcher"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"researcher"</span><span style="color:#E1E4E8">},  </span><span style="color:#6A737D"># Priority 8</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Editor"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"editor"</span><span style="color:#E1E4E8">},          </span><span style="color:#6A737D"># Priority 6</span></span>
<span class="line"><span style="color:#E1E4E8">    {</span><span style="color:#9ECBFF">"name"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Summarizer"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"summarizer"</span><span style="color:#E1E4E8">},  </span><span style="color:#6A737D"># Priority 5</span></span>
<span class="line"><span style="color:#E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> agent </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> agents:</span></span>
<span class="line"><span style="color:#E1E4E8">    priority </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> get_priority_for_role(agent[</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">    response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">        model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">        messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Task for </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">agent[</span><span style="color:#9ECBFF">'name'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">}],</span></span>
<span class="line"><span style="color:#FFAB70">        priority</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">priority</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span>
<span class="line"><span style="color:#6A737D">    # User-facing agents get priority, background tasks wait</span></span>
<span class="line"></span></code></pre>
<p><strong>Priority levels available:</strong></p>
<ul>
<li><code>Priority.CRITICAL</code> (10) - Mission-critical</li>
<li><code>Priority.URGENT</code> (9) - Fast response needed</li>
<li><code>Priority.HIGH</code> (7) - Important tasks</li>
<li><code>Priority.NORMAL</code> (5) - Default</li>
<li><code>Priority.LOW</code> (3) - Background tasks</li>
<li><code>Priority.BATCH</code> (1) - Can wait</li>
</ul>
<hr>
<h3 id="4-model-sharding-with-llamacpp-large-models">4. Model Sharding with llama.cpp (Large Models)</h3>
<p><strong>Run models larger than your biggest GPU</strong> by distributing layers across multiple machines.</p>
<h4 id="when-to-use-model-sharding">When to Use Model Sharding</h4>
<p>Use model sharding when:</p>
<ul>
<li>âœ… Model doesnâ€™t fit on your largest GPU (e.g., 70B models on 16GB GPUs)</li>
<li>âœ… You have multiple machines with network connectivity</li>
<li>âœ… You can tolerate slower inference for capability</li>
</ul>
<p>Donâ€™t use sharding when:</p>
<ul>
<li>âŒ Model fits on a single GPU (use task distribution instead)</li>
<li>âŒ You need maximum inference speed</li>
<li>âŒ Network latency is high (>10ms between machines)</li>
</ul>
<h4 id="quick-start-auto-setup-easiest">Quick Start: Auto-Setup (Easiest)</h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.sync_wrapper </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> HybridRouter, OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># SOLLOL handles all setup automatically</span></span>
<span class="line"><span style="color:#E1E4E8">router </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> HybridRouter(</span></span>
<span class="line"><span style="color:#FFAB70">    ollama_pool</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">OllamaPool.auto_configure(),</span></span>
<span class="line"><span style="color:#FFAB70">    enable_distributed</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Enable model sharding</span></span>
<span class="line"><span style="color:#FFAB70">    auto_setup_rpc</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,      </span><span style="color:#6A737D"># Auto-configure RPC backends</span></span>
<span class="line"><span style="color:#FFAB70">    num_rpc_backends</span><span style="color:#F97583">=</span><span style="color:#79B8FF">3</span><span style="color:#6A737D">        # Distribute across 3 machines</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Use large model that doesn't fit on one machine</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> router.route_request(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.1:70b"</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Automatically sharded across backends</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Explain quantum computing"</span><span style="color:#E1E4E8">}]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response[</span><span style="color:#9ECBFF">'message'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'content'</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span></code></pre>
<p><strong>What happens automatically:</strong></p>
<ol>
<li>SOLLOL discovers available RPC backends on your network</li>
<li>Extracts the GGUF model from Ollama storage</li>
<li>Starts llama-server coordinator with optimal settings</li>
<li>Distributes model layers across backends</li>
<li>Routes your request to the coordinator</li>
</ol>
<h4 id="rpc-server-auto-installation">RPC Server Auto-Installation</h4>
<p><strong>SOLLOL can automatically clone, build, and start llama.cpp RPC servers for you!</strong></p>
<p><strong>One-line installation:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.rpc_auto_setup </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> auto_setup_rpc_backends</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Automatically: clone â†’ build â†’ start RPC servers</span></span>
<span class="line"><span style="color:#E1E4E8">backends </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> auto_setup_rpc_backends(</span><span style="color:#FFAB70">num_backends</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># Output: [{'host': '127.0.0.1', 'port': 50052}, {'host': '127.0.0.1', 'port': 50053}]</span></span>
<span class="line"></span></code></pre>
<p><strong>What this does:</strong></p>
<ol>
<li>âœ… Scans network for existing RPC servers</li>
<li>âœ… If none found: clones llama.cpp to <code>~/llama.cpp</code></li>
<li>âœ… Builds llama.cpp with RPC support (<code>cmake -DGGML_RPC=ON</code>)</li>
<li>âœ… Starts RPC servers on ports 50052-50053</li>
<li>âœ… Returns ready-to-use backend list</li>
</ol>
<p><strong>CLI installation:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Full automated setup (clone + build + install systemd service)</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.setup_llama_cpp</span><span style="color:#79B8FF"> --all</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Or step by step</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.setup_llama_cpp</span><span style="color:#79B8FF"> --clone</span><span style="color:#6A737D">  # Clone llama.cpp</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.setup_llama_cpp</span><span style="color:#79B8FF"> --build</span><span style="color:#6A737D">  # Build with RPC support</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.setup_llama_cpp</span><span style="color:#79B8FF"> --start</span><span style="color:#6A737D">  # Start RPC server</span></span>
<span class="line"></span></code></pre>
<p><strong>Docker IP Resolution:</strong></p>
<p>SOLLOL automatically resolves Docker container IPs to accessible host IPs:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># If Docker container reports IP 172.17.0.5:11434</span></span>
<span class="line"><span style="color:#6A737D"># SOLLOL automatically resolves to:</span></span>
<span class="line"><span style="color:#6A737D"># â†’ 127.0.0.1:11434 (published port mapping)</span></span>
<span class="line"><span style="color:#6A737D"># â†’ host IP (if accessible)</span></span>
<span class="line"><span style="color:#6A737D"># â†’ Docker host gateway</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> is_docker_ip, resolve_docker_ip</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check if IP is Docker internal</span></span>
<span class="line"><span style="color:#E1E4E8">is_docker </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> is_docker_ip(</span><span style="color:#9ECBFF">"172.17.0.5"</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># True</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Resolve Docker IP to accessible IP</span></span>
<span class="line"><span style="color:#E1E4E8">accessible_ip </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> resolve_docker_ip(</span><span style="color:#9ECBFF">"172.17.0.5"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">port</span><span style="color:#F97583">=</span><span style="color:#79B8FF">11434</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># Returns: "127.0.0.1" or host IP</span></span>
<span class="line"></span></code></pre>
<p><strong>Network Discovery with Docker Support:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Auto-discover nodes (automatically resolves Docker IPs)</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Manual control</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.discovery </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> discover_ollama_nodes</span></span>
<span class="line"><span style="color:#E1E4E8">nodes </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> discover_ollama_nodes(</span><span style="color:#FFAB70">auto_resolve_docker</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>Multi-Node Production Setup:</strong></p>
<p>For distributed clusters, use systemd services on each node:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># On each RPC node</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> enable</span><span style="color:#9ECBFF"> llama-rpc@50052.service</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> start</span><span style="color:#9ECBFF"> llama-rpc@50052.service</span></span>
<span class="line"></span></code></pre>
<p>See <a href="https://github.com/BenevolentJoker-JohnL/FlockParser/blob/main/SOLLOL_RPC_SETUP.md">SOLLOL_RPC_SETUP.md</a> for complete installation guide.</p>
<h4 id="architecture-how-it-works">Architecture: How It Works</h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚    Llama 3.1 70B Model (40GB total)        â”‚</span></span>
<span class="line"><span>â”‚           Distributed Sharding             â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>                    â”‚</span></span>
<span class="line"><span>       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>       â”‚            â”‚            â”‚</span></span>
<span class="line"><span>       â–¼            â–¼            â–¼</span></span>
<span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚  Machine 1   â”‚ â”‚  Machine 2   â”‚ â”‚  Machine 3   â”‚</span></span>
<span class="line"><span>â”‚ Layers 0-26  â”‚ â”‚ Layers 27-53 â”‚ â”‚ Layers 54-79 â”‚</span></span>
<span class="line"><span>â”‚   (~13GB)    â”‚ â”‚   (~13GB)    â”‚ â”‚   (~13GB)    â”‚</span></span>
<span class="line"><span>â”‚ RPC Backend  â”‚ â”‚ RPC Backend  â”‚ â”‚ RPC Backend  â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>       â–²            â–²            â–²</span></span>
<span class="line"><span>       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>                    â”‚</span></span>
<span class="line"><span>         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>         â”‚ llama-server        â”‚</span></span>
<span class="line"><span>         â”‚ Coordinator         â”‚</span></span>
<span class="line"><span>         â”‚ (Port 18080)        â”‚</span></span>
<span class="line"><span>         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span></span></span></code></pre>
<h4 id="manual-setup-advanced">Manual Setup (Advanced)</h4>
<p>For explicit control over RPC backends:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.llama_cpp_coordinator </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> LlamaCppCoordinator</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.rpc_registry </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> RPCBackendRegistry</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 1. Register RPC backends explicitly</span></span>
<span class="line"><span style="color:#E1E4E8">registry </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> RPCBackendRegistry()</span></span>
<span class="line"><span style="color:#E1E4E8">registry.add_backend(</span><span style="color:#9ECBFF">"rpc_1"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"grpc://10.9.66.45:50052"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">registry.add_backend(</span><span style="color:#9ECBFF">"rpc_2"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"grpc://10.9.66.46:50052"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">registry.add_backend(</span><span style="color:#9ECBFF">"rpc_3"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"grpc://10.9.66.47:50052"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. Create coordinator</span></span>
<span class="line"><span style="color:#E1E4E8">coordinator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> LlamaCppCoordinator(</span></span>
<span class="line"><span style="color:#FFAB70">    coordinator_port</span><span style="color:#F97583">=</span><span style="color:#79B8FF">18080</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    rpc_backends</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">registry.get_all_backends(),</span></span>
<span class="line"><span style="color:#FFAB70">    context_size</span><span style="color:#F97583">=</span><span style="color:#79B8FF">4096</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    gpu_layers</span><span style="color:#F97583">=-</span><span style="color:#79B8FF">1</span><span style="color:#6A737D">  # Use all available GPU layers</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 3. Start and use</span></span>
<span class="line"><span style="color:#F97583">await</span><span style="color:#E1E4E8"> coordinator.start(</span><span style="color:#FFAB70">model_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.1:70b"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> coordinator.generate(</span></span>
<span class="line"><span style="color:#FFAB70">    prompt</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"Explain the theory of relativity"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    max_tokens</span><span style="color:#F97583">=</span><span style="color:#79B8FF">500</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h4 id="performance-expectations">Performance Expectations</h4>























<table><thead><tr><th>Model Size</th><th>Single GPU</th><th>Sharded (3 nodes)</th><th>Trade-off</th></tr></thead><tbody><tr><td><strong>13B</strong></td><td>âœ… 20 tok/s</td><td>âœ… 5 tok/s</td><td>-75% speed, works on 3Ã—smaller GPUs</td></tr><tr><td><strong>70B</strong></td><td>âŒ OOM</td><td>âš ï¸ 3-5 tok/s (est.)</td><td>Enables model that wonâ€™t run otherwise</td></tr></tbody></table>
<p><strong>Trade-offs:</strong></p>
<ul>
<li>ğŸŒ <strong>Startup</strong>: 2-5 minutes (model distribution + loading)</li>
<li>ğŸŒ <strong>Inference</strong>: ~4x slower than local (network overhead)</li>
<li>âœ… <strong>Capability</strong>: Run models that wonâ€™t fit on single GPU</li>
</ul>
<p><strong>Learn More:</strong></p>
<ul>
<li>ğŸ“– <a href="docs/llama_cpp_guide.md">Complete llama.cpp Guide</a> - Setup, optimization, troubleshooting</li>
<li>ğŸ’» <a href="examples/llama_cpp_distributed.py">Working Examples</a> - 5 complete examples including conversation, batch processing, error handling</li>
</ul>
<hr>
<h3 id="5-batch-processing-api">5. Batch Processing API</h3>
<p><strong>New in v0.7.0:</strong> RESTful API for asynchronous batch job management.</p>
<p>Submit large-scale batch operations (thousands of embeddings, bulk inference) and track progress via job IDs:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> requests</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Submit batch embedding job (up to 10,000 documents)</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.post(</span><span style="color:#9ECBFF">"http://localhost:11434/api/batch/embed"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">json</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#9ECBFF">    "model"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"nomic-embed-text"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "documents"</span><span style="color:#E1E4E8">: [</span><span style="color:#9ECBFF">"Document 1"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"Document 2"</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># Can be thousands</span></span>
<span class="line"><span style="color:#9ECBFF">    "metadata"</span><span style="color:#E1E4E8">: {</span><span style="color:#9ECBFF">"source"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"knowledge_base"</span><span style="color:#E1E4E8">}  </span><span style="color:#6A737D"># Optional metadata</span></span>
<span class="line"><span style="color:#E1E4E8">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">job_id </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> response.json()[</span><span style="color:#9ECBFF">"job_id"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Job submitted: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">job_id</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Poll for job status</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> time</span></span>
<span class="line"><span style="color:#F97583">while</span><span style="color:#79B8FF"> True</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#E1E4E8">    status </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.get(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"http://localhost:11434/api/batch/jobs/</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">job_id</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">).json()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">    progress </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> status[</span><span style="color:#9ECBFF">"progress"</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">"percent"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Progress: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">progress</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">%"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> status[</span><span style="color:#9ECBFF">"status"</span><span style="color:#E1E4E8">] </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "completed"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">        break</span></span>
<span class="line"><span style="color:#E1E4E8">    time.sleep(</span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Get results</span></span>
<span class="line"><span style="color:#E1E4E8">results </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.get(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"http://localhost:11434/api/batch/results/</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">job_id</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">).json()</span></span>
<span class="line"><span style="color:#E1E4E8">embeddings </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> results[</span><span style="color:#9ECBFF">"results"</span><span style="color:#E1E4E8">]  </span><span style="color:#6A737D"># List of embedding vectors</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Processed </span><span style="color:#79B8FF">{len</span><span style="color:#E1E4E8">(embeddings)</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> documents in </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">status[</span><span style="color:#9ECBFF">'duration_seconds'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">s"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>Available Batch Endpoints:</strong></p>
<ul>
<li><code>POST /api/batch/embed</code> - Submit batch embedding job</li>
<li><code>GET /api/batch/jobs/{job_id}</code> - Get job status</li>
<li><code>GET /api/batch/results/{job_id}</code> - Get job results</li>
<li><code>GET /api/batch/jobs?limit=100</code> - List recent jobs</li>
<li><code>DELETE /api/batch/jobs/{job_id}</code> - Cancel job</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Embedding large document collections (thousands of documents)</li>
<li>Bulk inference for batch predictions</li>
<li>Background processing without blocking</li>
<li>Long-running operations with progress tracking</li>
</ul>
<hr>
<h3 id="6-sollol-detection">6. SOLLOL Detection</h3>
<p><strong>New in v0.3.6:</strong> Detect if SOLLOL is running vs native Ollama.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> requests</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">def</span><span style="color:#B392F0"> is_sollol</span><span style="color:#E1E4E8">(url</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:11434"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#9ECBFF">    """Check if SOLLOL is running at the given URL."""</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Method 1: Check X-Powered-By header</span></span>
<span class="line"><span style="color:#E1E4E8">    response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.get(url)</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> response.headers.get(</span><span style="color:#9ECBFF">"X-Powered-By"</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "SOLLOL"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> True</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D">    # Method 2: Check health endpoint</span></span>
<span class="line"><span style="color:#E1E4E8">    response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.get(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">url</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">/api/health"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    data </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> response.json()</span></span>
<span class="line"><span style="color:#F97583">    if</span><span style="color:#E1E4E8"> data.get(</span><span style="color:#9ECBFF">"service"</span><span style="color:#E1E4E8">) </span><span style="color:#F97583">==</span><span style="color:#9ECBFF"> "SOLLOL"</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#F97583">        return</span><span style="color:#79B8FF"> True</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#79B8FF"> False</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Use it</span></span>
<span class="line"><span style="color:#F97583">if</span><span style="color:#E1E4E8"> is_sollol(</span><span style="color:#9ECBFF">"http://localhost:11434"</span><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"âœ“ SOLLOL detected - using intelligent routing"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#F97583">else</span><span style="color:#E1E4E8">:</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#9ECBFF">"Native Ollama detected"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>Why this matters:</strong></p>
<ul>
<li>Enables graceful fallback in client applications</li>
<li>Makes SOLLOL a true drop-in replacement</li>
<li>Clients can auto-detect and use SOLLOL features when available</li>
</ul>
<hr>
<h3 id="7-production-gateway">7. Production Gateway</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#79B8FF"> SOLLOL</span><span style="color:#E1E4E8">, SOLLOLConfig</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Full production setup</span></span>
<span class="line"><span style="color:#E1E4E8">config </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SOLLOLConfig(</span></span>
<span class="line"><span style="color:#FFAB70">    ray_workers</span><span style="color:#F97583">=</span><span style="color:#79B8FF">4</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    dask_workers</span><span style="color:#F97583">=</span><span style="color:#79B8FF">2</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    hosts</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"gpu-1:11434"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"gpu-2:11434"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"cpu-1:11434"</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#FFAB70">    gateway_port</span><span style="color:#F97583">=</span><span style="color:#79B8FF">8000</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    metrics_port</span><span style="color:#F97583">=</span><span style="color:#79B8FF">9090</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">sollol </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SOLLOL(config)</span></span>
<span class="line"><span style="color:#E1E4E8">sollol.start()  </span><span style="color:#6A737D"># Blocks and runs gateway</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Access via HTTP:</span></span>
<span class="line"><span style="color:#6A737D"># curl http://localhost:8000/api/chat -d '{...}'</span></span>
<span class="line"><span style="color:#6A737D"># curl http://localhost:8000/api/stats</span></span>
<span class="line"><span style="color:#6A737D"># curl http://localhost:8000/api/dashboard</span></span>
<span class="line"></span></code></pre>
<hr>
<h2 id="-use-cases">ğŸ“ Use Cases</h2>
<h3 id="1-multi-agent-ai-systems-synapticllamas-crewai-autogpt">1. Multi-Agent AI Systems (SynapticLlamas, CrewAI, AutoGPT)</h3>
<p><strong>Problem</strong>: Running 10 agents sequentially takes 10x longer than necessary.</p>
<p><strong>Solution</strong>: SOLLOL distributes agents across nodes in parallel.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Before: Sequential execution on one node</span></span>
<span class="line"><span style="color:#6A737D"># After: Parallel execution with SOLLOL</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"><span style="color:#E1E4E8">agents </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> asyncio.gather(</span><span style="color:#F97583">*</span><span style="color:#E1E4E8">[</span></span>
<span class="line"><span style="color:#E1E4E8">    pool.chat(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">agent_prompts[i])</span></span>
<span class="line"><span style="color:#F97583">    for</span><span style="color:#E1E4E8"> i </span><span style="color:#F97583">in</span><span style="color:#79B8FF"> range</span><span style="color:#E1E4E8">(</span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#6A737D"># Speedup depends on number of available nodes and their capacity</span></span>
<span class="line"></span></code></pre>
<h3 id="2-large-model-inference">2. Large Model Inference</h3>
<p><strong>Problem</strong>: Your model doesnâ€™t fit in available VRAM.</p>
<p><strong>Solution</strong>: SOLLOL can shard models across multiple machines via llama.cpp.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Distribute model across multiple nodes</span></span>
<span class="line"><span style="color:#6A737D"># Note: Verified with 13B models; larger models not extensively tested</span></span>
<span class="line"><span style="color:#E1E4E8">router </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> HybridRouter(</span></span>
<span class="line"><span style="color:#FFAB70">    enable_distributed</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    num_rpc_backends</span><span style="color:#F97583">=</span><span style="color:#79B8FF">4</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># Trade-off: Slower startup/inference but enables running larger models</span></span>
<span class="line"></span></code></pre>
<h3 id="3-mixed-workloads">3. Mixed Workloads</h3>
<p><strong>Problem</strong>: Different tasks need different resources.</p>
<p><strong>Solution</strong>: SOLLOL routes each task to the optimal node.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Heavy generation â†’ GPU node</span></span>
<span class="line"><span style="color:#E1E4E8">chat </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2:70b"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Fast embeddings â†’ CPU node</span></span>
<span class="line"><span style="color:#E1E4E8">embeddings </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.embed(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"nomic-embed-text"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># SOLLOL automatically routes each to the best available node</span></span>
<span class="line"></span></code></pre>
<h3 id="4-high-availability-production">4. High Availability Production</h3>
<p><strong>Problem</strong>: Node failures break your service.</p>
<p><strong>Solution</strong>: SOLLOL auto-fails over and recovers.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Node A fails mid-request</span></span>
<span class="line"><span style="color:#6A737D"># âœ… SOLLOL automatically:</span></span>
<span class="line"><span style="color:#6A737D"># 1. Detects failure</span></span>
<span class="line"><span style="color:#6A737D"># 2. Retries on Node B</span></span>
<span class="line"><span style="color:#6A737D"># 3. Marks Node A as degraded</span></span>
<span class="line"><span style="color:#6A737D"># 4. Periodically re-checks Node A</span></span>
<span class="line"><span style="color:#6A737D"># 5. Restores Node A when healthy</span></span>
<span class="line"></span></code></pre>
<h4 id="simulate-failure--recovery"><strong>Simulate Failure &#x26; Recovery</strong></h4>
<p>Want to see SOLLOLâ€™s automatic failover in action? Run the included simulation:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> test_failure_recovery.py</span></span>
<span class="line"></span></code></pre>
<p><strong>What the simulation does:</strong></p>
<ol>
<li>Starts 3 mock Ollama nodes</li>
<li>Sends baseline requests (all nodes healthy)</li>
<li><strong>Kills node #1 mid-execution</strong></li>
<li>Continues sending requests (SOLLOL routes around failed node)</li>
<li>Restores node #1</li>
<li>Resumes sending requests (traffic returns to recovered node)</li>
</ol>
<p><strong>Expected output:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>STEP 1: Starting Mock Nodes</span></span>
<span class="line"><span>âœ… Started 3 mock nodes</span></span>
<span class="line"><span></span></span>
<span class="line"><span>BASELINE: Requests with all nodes healthy</span></span>
<span class="line"><span>  Request 1: âœ“ Routed to localhost:21434</span></span>
<span class="line"><span>  Request 2: âœ“ Routed to localhost:21435</span></span>
<span class="line"><span>  ...</span></span>
<span class="line"><span></span></span>
<span class="line"><span>STEP 3: Simulating Node Failure (killing node 0)</span></span>
<span class="line"><span>Killing node on port 21434...</span></span>
<span class="line"><span>âœ… Node 21434 terminated</span></span>
<span class="line"><span></span></span>
<span class="line"><span>STEP 4: Requests after node failure (observe failover)</span></span>
<span class="line"><span>  Request 1: âœ“ Routed to localhost:21435  â† Automatically avoided dead node</span></span>
<span class="line"><span>  Request 2: âœ“ Routed to localhost:21436</span></span>
<span class="line"><span>  ...</span></span>
<span class="line"><span></span></span>
<span class="line"><span>STEP 5: Simulating Node Recovery</span></span>
<span class="line"><span>âœ… Node 21434 recovered successfully</span></span>
<span class="line"><span></span></span>
<span class="line"><span>âœ… Key Observations:</span></span>
<span class="line"><span>  1. Requests succeeded even after node failure</span></span>
<span class="line"><span>  2. SOLLOL automatically routed around the dead node</span></span>
<span class="line"><span>  3. Node recovered and rejoined the pool</span></span>
<span class="line"><span>  4. Traffic resumed to recovered node</span></span>
<span class="line"><span></span></span></code></pre>
<p>This demonstrates SOLLOLâ€™s production-grade resilience without needing real infrastructure.</p>
<hr>
<h2 id="-performance--benchmarks">ğŸ“Š Performance &#x26; Benchmarks</h2>
<h3 id="validation-status">Validation Status</h3>
<p><strong>Whatâ€™s Been Validated âœ…</strong></p>
<ul>
<li>Single-node baseline performance measured</li>
<li>Code exists and is reviewable (75+ modules)</li>
<li>Tests pass in CI (57 tests, coverage tracked)</li>
<li>Architecture implements intelligent routing</li>
</ul>
<p><strong>What Needs Validation âš ï¸</strong></p>
<ul>
<li>Comparative benchmarks (SOLLOL vs round-robin)</li>
<li>Multi-node performance improvements</li>
<li>Real-world latency/throughput gains</li>
</ul>
<p>ğŸ“– <strong>See <a href="BENCHMARKING.md">BENCHMARKING.md</a> for complete validation roadmap and how to run comparative tests.</strong></p>
<hr>
<h3 id="measured-baseline-performance">Measured Baseline Performance</h3>
<p><strong>Single Ollama Node</strong> (llama3.2-3B, 50 requests, concurrency=5):</p>
<ul>
<li>âœ… <strong>Success Rate:</strong> 100%</li>
<li>âš¡ <strong>Throughput:</strong> 0.51 req/s</li>
<li>ğŸ“ˆ <strong>Average Latency:</strong> 5,659 ms</li>
<li>ğŸ“ˆ <strong>P95 Latency:</strong> 11,299 ms</li>
<li>ğŸ“ˆ <strong>P99 Latency:</strong> 12,259 ms</li>
</ul>
<p><strong>Hardware:</strong> Single Ollama instance with 75+ models loaded
<strong>Data:</strong> See <a href="benchmarks/results/"><code>benchmarks/results/</code></a> for raw JSON</p>
<p><strong>Run Your Own:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Baseline test (no cluster needed)</span></span>
<span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> benchmarks/simple_ollama_benchmark.py</span><span style="color:#9ECBFF"> llama3.2</span><span style="color:#79B8FF"> 50</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Comparative test (requires docker-compose)</span></span>
<span class="line"><span style="color:#B392F0">docker-compose</span><span style="color:#9ECBFF"> up</span><span style="color:#79B8FF"> -d</span></span>
<span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> benchmarks/run_benchmarks.py</span><span style="color:#79B8FF"> --sollol-url</span><span style="color:#9ECBFF"> http://localhost:8000</span><span style="color:#79B8FF"> --duration</span><span style="color:#79B8FF"> 60</span></span>
<span class="line"></span></code></pre>
<hr>
<h3 id="projected-performance-unvalidated">Projected Performance (Unvalidated)</h3>
<p><strong>Note:</strong> These are architectural projections, not measured results. Requires multi-node cluster setup for validation.</p>
<p><strong>Theory:</strong> With N nodes and parallelizable workload:</p>
<ul>
<li>Task distribution can approach NÃ— parallelization (limited by request rate)</li>
<li>Intelligent routing should reduce tail latencies vs random selection</li>
<li>Resource-aware placement reduces contention and failures</li>
</ul>
<p><strong>Reality:</strong> Requires multi-node cluster validation. See <a href="BENCHMARKING.md">BENCHMARKING.md</a> for test procedure and <a href="CODE_WALKTHROUGH.md">CODE_WALKTHROUGH.md</a> for implementation details.</p>
<h3 id="model-sharding-performance">Model Sharding Performance</h3>























<table><thead><tr><th>Model</th><th>Single 24GB GPU</th><th>SOLLOL (3Ã—16GB)</th><th>Status</th></tr></thead><tbody><tr><td><strong>13B</strong></td><td>âœ… ~20 tok/s</td><td>âœ… ~5 tok/s</td><td>âœ… Verified working</td></tr><tr><td><strong>70B</strong></td><td>âŒ OOM</td><td>âš ï¸ Estimated ~3-5 tok/s</td><td>âš ï¸ Not extensively tested</td></tr></tbody></table>
<p><strong>When to use sharding</strong>: When model doesnâ€™t fit on your largest GPU. You trade speed for capability.</p>
<p><strong>Performance trade-offs</strong>: Distributed inference is 2-5 minutes slower to start and ~4x slower for inference compared to local. Use only when necessary.</p>
<h3 id="overhead">Overhead</h3>
<ul>
<li><strong>Routing decision</strong>: ~5-10ms (tested with 5-10 nodes)</li>
<li><strong>Network overhead</strong>: Varies by network (typically 5-20ms)</li>
<li><strong>Total added latency</strong>: ~20-50ms</li>
<li><strong>Benefit</strong>: Better resource utilization + automatic failover</li>
</ul>
<hr>
<h2 id="ï¸-advanced-configuration">ğŸ› ï¸ Advanced Configuration</h2>
<h3 id="custom-routing-strategy">Custom Routing Strategy</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool(</span></span>
<span class="line"><span style="color:#FFAB70">    nodes</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span></span>
<span class="line"><span style="color:#E1E4E8">        {</span><span style="color:#9ECBFF">"host"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"gpu-1.local"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"port"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">11434</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"priority"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">},  </span><span style="color:#6A737D"># Prefer this</span></span>
<span class="line"><span style="color:#E1E4E8">        {</span><span style="color:#9ECBFF">"host"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"gpu-2.local"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"port"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">11434</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"priority"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">5</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#E1E4E8">        {</span><span style="color:#9ECBFF">"host"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"cpu-1.local"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"port"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">11434</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"priority"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">1</span><span style="color:#E1E4E8">},   </span><span style="color:#6A737D"># Last resort</span></span>
<span class="line"><span style="color:#E1E4E8">    ],</span></span>
<span class="line"><span style="color:#FFAB70">    enable_intelligent_routing</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    enable_hedging</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Duplicate critical requests</span></span>
<span class="line"><span style="color:#FFAB70">    max_queue_size</span><span style="color:#F97583">=</span><span style="color:#79B8FF">100</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h3 id="priority-based-scheduling">Priority-Based Scheduling</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Critical user-facing request</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#FFAB70">    priority</span><span style="color:#F97583">=</span><span style="color:#79B8FF">10</span><span style="color:#6A737D">  # Highest priority</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Background batch job</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#FFAB70">    priority</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1</span><span style="color:#6A737D">  # Lowest priority</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># SOLLOL ensures high-priority requests jump the queue</span></span>
<span class="line"></span></code></pre>
<h3 id="observability--monitoring">Observability &#x26; Monitoring</h3>
<h4 id="zero-config-auto-registration"><strong>Zero-Config Auto-Registration</strong> ğŸ¯</h4>
<p>SOLLOL provides <strong>automatic observability</strong> with zero configuration required. All applications automatically register with the dashboard when they create an <code>OllamaPool</code>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Creates pool AND auto-registers with dashboard (if running)</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"><span style="color:#6A737D"># âœ… Application automatically appears in dashboard at http://localhost:8080</span></span>
<span class="line"></span></code></pre>
<p><strong>How it works:</strong></p>
<ol>
<li><code>OllamaPool</code> automatically detects if a dashboard is running on port 8080</li>
<li>Auto-discovers RPC backends and Ollama nodes</li>
<li>Registers application with metadata (node count, GPU info, etc.)</li>
<li>Sends periodic heartbeats to maintain â€œaliveâ€ status</li>
<li>No manual <code>DashboardClient</code> setup needed!</li>
</ol>
<p><strong>Architecture:</strong></p>
<ul>
<li><strong>ONE persistent dashboard service</strong> runs independently</li>
<li><strong>Multiple applications</strong> (SynapticLlamas, FlockParser, etc.) auto-register</li>
<li><strong>Dashboard survives</strong> application exits</li>
<li><strong>Zero-config</strong> auto-discovery of nodes and RPC backends</li>
</ul>
<h4 id="custom-application-names-ï¸"><strong>Custom Application Names</strong> ğŸ·ï¸</h4>
<p>By default, applications register as â€œOllamaPool (hostname)â€. To give your application a custom name in the dashboard:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Register with custom application name</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool(</span></span>
<span class="line"><span style="color:#FFAB70">    nodes</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"host"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"localhost"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"port"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">11434</span><span style="color:#E1E4E8">}],</span></span>
<span class="line"><span style="color:#FFAB70">    enable_intelligent_routing</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    app_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"MyApplication"</span><span style="color:#6A737D">  # Shows as "MyApplication" in dashboard</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<p><strong>Example - Multi-application setup:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Application 1: FlockParser</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure(</span><span style="color:#FFAB70">app_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"FlockParser"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># Dashboard shows: "FlockParser"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Application 2: SynapticLlamas</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.dashboard_client </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DashboardClient</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DashboardClient(</span></span>
<span class="line"><span style="color:#FFAB70">    app_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"SynapticLlamas"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    router_type</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"IntelligentRouter"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    version</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"1.0.0"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    dashboard_url</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:8080"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    metadata</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">{</span><span style="color:#9ECBFF">"agents"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">3</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"distributed"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#FFAB70">    auto_register</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># Dashboard shows: "SynapticLlamas"</span></span>
<span class="line"></span></code></pre>
<p><strong>Why use custom names?</strong></p>
<ul>
<li>Distinguish between multiple applications using SOLLOL</li>
<li>Better visibility in multi-tenant environments</li>
<li>Easier debugging and monitoring</li>
<li>Professional dashboard presentation</li>
</ul>
<h4 id="manualprogrammatic-registration"><strong>Manual/Programmatic Registration</strong> ğŸ”§</h4>
<p>For applications that donâ€™t use <code>OllamaPool</code> or need custom registration logic, use <code>DashboardClient</code> directly:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.dashboard_client </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DashboardClient</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Create dashboard client with custom metadata</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DashboardClient(</span></span>
<span class="line"><span style="color:#FFAB70">    app_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"CustomApplication"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    router_type</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"CustomRouter"</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Or "OllamaPool", "HybridRouter", etc.</span></span>
<span class="line"><span style="color:#FFAB70">    version</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"1.0.0"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    dashboard_url</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:8080"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    metadata</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#6A737D">        # Custom metadata shown in dashboard</span></span>
<span class="line"><span style="color:#9ECBFF">        "nodes"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">5</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "distributed"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "custom_field"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"value"</span></span>
<span class="line"><span style="color:#E1E4E8">    },</span></span>
<span class="line"><span style="color:#FFAB70">    auto_register</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#6A737D">  # Registers immediately</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Dashboard client automatically sends heartbeats every 5 seconds</span></span>
<span class="line"><span style="color:#6A737D"># to keep application status as "active"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># When application exits, clean up:</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client.close()  </span><span style="color:#6A737D"># Stops heartbeat thread</span></span>
<span class="line"></span></code></pre>
<p><strong>Advanced: Custom Heartbeat Logic</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.dashboard_client </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> DashboardClient</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> time</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Create client without auto-registration</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> DashboardClient(</span></span>
<span class="line"><span style="color:#FFAB70">    app_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"BackgroundWorker"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    router_type</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"WorkerPool"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    version</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"2.0.0"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    dashboard_url</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:8080"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    metadata</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">{</span><span style="color:#9ECBFF">"worker_count"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">10</span><span style="color:#E1E4E8">},</span></span>
<span class="line"><span style="color:#FFAB70">    auto_register</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#6A737D">  # Don't register yet</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Register when ready</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client.register()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Update metadata dynamically</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client.update_metadata({</span><span style="color:#9ECBFF">"worker_count"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">15</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"status"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"processing"</span><span style="color:#E1E4E8">})</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Send manual heartbeat</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client.heartbeat()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Application logic here...</span></span>
<span class="line"><span style="color:#E1E4E8">time.sleep(</span><span style="color:#79B8FF">60</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Deregister when done</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client.deregister()</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_client.close()</span></span>
<span class="line"></span></code></pre>
<p><strong>Use cases for manual registration:</strong></p>
<ul>
<li>Custom routers or load balancers</li>
<li>Background workers or daemons</li>
<li>Applications that need dynamic metadata updates</li>
<li>Testing and debugging</li>
<li>Applications without OllamaPool</li>
</ul>
<h4 id="registration-methods-comparison"><strong>Registration Methods Comparison</strong> ğŸ“Š</h4>





























<table><thead><tr><th>Method</th><th>Use Case</th><th>Complexity</th><th>Customization</th></tr></thead><tbody><tr><td><strong>Auto-registration</strong></td><td>Standard SOLLOL applications</td><td>âœ… Zero config</td><td>Limited (app_name only)</td></tr><tr><td><strong>Custom app_name</strong></td><td>Multiple apps, better naming</td><td>âœ… One parameter</td><td>App name</td></tr><tr><td><strong>Manual DashboardClient</strong></td><td>Custom applications</td><td>âš ï¸ More code</td><td>Full control</td></tr></tbody></table>
<p><strong>Quick decision guide:</strong></p>
<ul>
<li>Using <code>OllamaPool</code>? â†’ Use <code>app_name</code> parameter</li>
<li>Need custom metadata? â†’ Use <code>DashboardClient</code> directly</li>
<li>Need dynamic updates? â†’ Use <code>DashboardClient</code> with manual heartbeats</li>
<li>Just want it to work? â†’ Use auto-registration (default)</li>
</ul>
<h4 id="persistent-dashboard-service"><strong>Persistent Dashboard Service</strong></h4>
<p>Start the persistent dashboard once (survives application exits):</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Start dashboard service (runs until stopped)</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.dashboard_service</span><span style="color:#79B8FF"> --port</span><span style="color:#79B8FF"> 8080</span><span style="color:#79B8FF"> --redis-url</span><span style="color:#9ECBFF"> redis://localhost:6379</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Or run in background</span></span>
<span class="line"><span style="color:#B392F0">nohup</span><span style="color:#9ECBFF"> python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.dashboard_service</span><span style="color:#79B8FF"> --port</span><span style="color:#79B8FF"> 8080</span><span style="color:#79B8FF"> --redis-url</span><span style="color:#9ECBFF"> redis://localhost:6379</span><span style="color:#F97583"> ></span><span style="color:#9ECBFF"> /tmp/dashboard_service.log</span><span style="color:#F97583"> 2>&#x26;1</span><span style="color:#E1E4E8"> &#x26;</span></span>
<span class="line"></span></code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>ğŸ“Š <strong>Real-time metrics</strong>: System status, latency, success rate, GPU memory, Ray workers</li>
<li>ğŸ“œ <strong>Live log streaming</strong>: WebSocket-based log tailing (via Redis pub/sub)</li>
<li>ğŸŒ <strong>Activity monitoring</strong>: Ollama server and llama.cpp RPC activity</li>
<li>ğŸ”· <strong>Embedded Ray dashboard</strong>: Task-level distributed tracing</li>
<li>ğŸ“ˆ <strong>Embedded Dask dashboard</strong>: Performance profiling and task graphs</li>
<li>ğŸ” <strong>Auto-discovery</strong>: Automatically discovers Ollama nodes and RPC backends when no router context</li>
</ul>
<h4 id="embedded-dashboard-alternative"><strong>Embedded Dashboard (Alternative)</strong></h4>
<p>Applications can also start their own embedded dashboards:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> run_unified_dashboard</span></span>
<span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> threading</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Start embedded dashboard with router context</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_thread </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> threading.Thread(</span></span>
<span class="line"><span style="color:#FFAB70">    target</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">run_unified_dashboard,</span></span>
<span class="line"><span style="color:#FFAB70">    kwargs</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#9ECBFF">        "router"</span><span style="color:#E1E4E8">: pool,  </span><span style="color:#6A737D"># Provides node/backend context</span></span>
<span class="line"><span style="color:#9ECBFF">        "dashboard_port"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">8080</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "host"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"0.0.0.0"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">        "enable_dask"</span><span style="color:#E1E4E8">: </span><span style="color:#79B8FF">False</span></span>
<span class="line"><span style="color:#E1E4E8">    },</span></span>
<span class="line"><span style="color:#FFAB70">    daemon</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">dashboard_thread.start()</span></span>
<span class="line"></span></code></pre>
<p><strong>Environment Variables</strong> (configure before initializing):</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Disable dashboard (default: true)</span></span>
<span class="line"><span style="color:#F97583">export</span><span style="color:#E1E4E8"> SOLLOL_DASHBOARD</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">false</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Change dashboard port (default: 8080)</span></span>
<span class="line"><span style="color:#F97583">export</span><span style="color:#E1E4E8"> SOLLOL_DASHBOARD_PORT</span><span style="color:#F97583">=</span><span style="color:#79B8FF">9090</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Disable Dask dashboard integration (default: true)</span></span>
<span class="line"><span style="color:#F97583">export</span><span style="color:#E1E4E8"> SOLLOL_DASHBOARD_DASK</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">false</span></span>
<span class="line"></span></code></pre>
<h4 id="multi-application-pattern"><strong>Multi-Application Pattern</strong> âœ¨</h4>
<p>The persistent dashboard service enables multiple applications to share observability:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Terminal 1: Start persistent dashboard</span></span>
<span class="line"><span style="color:#B392F0">python3</span><span style="color:#79B8FF"> -m</span><span style="color:#9ECBFF"> sollol.dashboard_service</span><span style="color:#79B8FF"> --port</span><span style="color:#79B8FF"> 8080</span><span style="color:#79B8FF"> --redis-url</span><span style="color:#9ECBFF"> redis://localhost:6379</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Terminal 2: Start application 1</span></span>
<span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> my_app1.py</span><span style="color:#6A737D">  # Auto-registers with dashboard</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Terminal 3: Start application 2</span></span>
<span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> my_app2.py</span><span style="color:#6A737D">  # Also auto-registers</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Visit http://localhost:8080 to see both applications!</span></span>
<span class="line"></span></code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Single dashboard for all SOLLOL-based applications</li>
<li>Dashboard stays running when applications exit</li>
<li>Aggregated logs from all applications (via Redis pub/sub)</li>
<li>Centralized observability for distributed systems</li>
</ul>
<h4 id="programmatic-stats-access"><strong>Programmatic Stats Access</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Get detailed stats</span></span>
<span class="line"><span style="color:#E1E4E8">stats </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.get_stats()</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Total requests: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'total_requests'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Average latency: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'avg_latency_ms'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">ms"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Success rate: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'success_rate'</span><span style="color:#E1E4E8">]</span><span style="color:#F97583">:.2%</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Per-node breakdown</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> host, metrics </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> stats[</span><span style="color:#9ECBFF">'hosts'</span><span style="color:#E1E4E8">].items():</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">host</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">metrics[</span><span style="color:#9ECBFF">'latency_ms'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">ms, </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">metrics[</span><span style="color:#9ECBFF">'success_rate'</span><span style="color:#E1E4E8">]</span><span style="color:#F97583">:.2%</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h4 id="prometheus-metrics"><strong>Prometheus Metrics</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Prometheus metrics endpoint</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://localhost:9090/metrics</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># sollol_requests_total{host="gpu-1:11434",model="llama3.2"} 1234</span></span>
<span class="line"><span style="color:#6A737D"># sollol_latency_seconds{host="gpu-1:11434"} 0.234</span></span>
<span class="line"><span style="color:#6A737D"># sollol_success_rate{host="gpu-1:11434"} 0.98</span></span>
<span class="line"></span></code></pre>
<hr>
<h2 id="-integration-examples">ğŸ”Œ Integration Examples</h2>
<h3 id="-integration-with-synapticllamas--flockparser">ğŸ”— Integration with SynapticLlamas &#x26; FlockParser</h3>
<p>SOLLOL is the <strong>distributed inference platform</strong> for the complete AI ecosystem, powering both <strong><a href="https://github.com/BenevolentJoker-JohnL/SynapticLlamas">SynapticLlamas</a></strong> (multi-agent orchestration) and <strong><a href="https://github.com/BenevolentJoker-JohnL/FlockParser">FlockParser</a></strong> (document RAG).</p>
<h3 id="the-complete-stack"><strong>The Complete Stack</strong></h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚              SynapticLlamas (v0.1.0+)                       â”‚</span></span>
<span class="line"><span>â”‚          Multi-Agent System &#x26; Orchestration                 â”‚</span></span>
<span class="line"><span>â”‚  â€¢ Research agents  â€¢ Editor agents  â€¢ Storyteller agents  â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>            â”‚                                    â”‚</span></span>
<span class="line"><span>            â”‚ RAG Queries                        â”‚ Distributed</span></span>
<span class="line"><span>            â”‚ (with pre-computed embeddings)     â”‚ Inference</span></span>
<span class="line"><span>            â”‚                                    â”‚</span></span>
<span class="line"><span>     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>     â”‚  FlockParser    â”‚              â”‚      SOLLOL          â”‚</span></span>
<span class="line"><span>     â”‚  API (v1.0.4+)  â”‚              â”‚  Load Balancer       â”‚</span></span>
<span class="line"><span>     â”‚  Port: 8000     â”‚              â”‚  (v0.9.31+)          â”‚</span></span>
<span class="line"><span>     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>            â”‚                                    â”‚</span></span>
<span class="line"><span>            â”‚ ChromaDB                          â”‚ Intelligent</span></span>
<span class="line"><span>            â”‚ Vector Store                      â”‚ GPU/CPU Routing</span></span>
<span class="line"><span>            â”‚                                    â”‚</span></span>
<span class="line"><span>     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>     â”‚  Knowledge Base â”‚              â”‚  Ollama Nodes        â”‚</span></span>
<span class="line"><span>     â”‚  41 Documents   â”‚              â”‚  (Distributed)       â”‚</span></span>
<span class="line"><span>     â”‚  6,141 Chunks   â”‚              â”‚  GPU + CPU           â”‚</span></span>
<span class="line"><span>     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span></span></span></code></pre>
<h3 id="why-this-integration-matters"><strong>Why This Integration Matters</strong></h3>

























<table><thead><tr><th>Component</th><th>Role</th><th>Key Feature</th></tr></thead><tbody><tr><td><strong>SOLLOL</strong></td><td>Distributed Inference</td><td>Intelligent GPU/CPU routing with load balancing</td></tr><tr><td><strong>SynapticLlamas</strong></td><td>Multi-Agent Orchestration</td><td>Research, Editor, Storyteller agents</td></tr><tr><td><strong>FlockParser</strong></td><td>Document RAG &#x26; Knowledge Base</td><td>ChromaDB vector store with 6,141+ chunks</td></tr></tbody></table>
<h3 id="quick-start-complete-ecosystem"><strong>Quick Start: Complete Ecosystem</strong></h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Install all three packages (auto-installs dependencies)</span></span>
<span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> synaptic-llamas</span><span style="color:#6A737D">  # Pulls in flockparser>=1.0.4 and sollol>=0.9.31</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Start FlockParser API</span></span>
<span class="line"><span style="color:#B392F0">flockparse</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Run SynapticLlamas with SOLLOL + FlockParser integration</span></span>
<span class="line"><span style="color:#B392F0">synaptic-llamas</span><span style="color:#79B8FF"> --interactive</span><span style="color:#79B8FF"> --distributed</span></span>
<span class="line"></span></code></pre>
<h3 id="integration-example-load-balanced-rag"><strong>Integration Example: Load Balanced RAG</strong></h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> flockparser_adapter </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> FlockParserAdapter</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Initialize SOLLOL for distributed inference</span></span>
<span class="line"><span style="color:#E1E4E8">sollol </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Initialize FlockParser adapter</span></span>
<span class="line"><span style="color:#E1E4E8">flockparser </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> FlockParserAdapter(</span><span style="color:#9ECBFF">"http://localhost:8000"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">remote_mode</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Step 1: Generate embedding using SOLLOL (load balanced!)</span></span>
<span class="line"><span style="color:#E1E4E8">user_query </span><span style="color:#F97583">=</span><span style="color:#9ECBFF"> "What does research say about quantum entanglement?"</span></span>
<span class="line"><span style="color:#E1E4E8">embedding </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> sollol.embed(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"mxbai-embed-large"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    input</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">user_query</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># SOLLOL routes to fastest GPU automatically</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Step 2: Query FlockParser with pre-computed embedding</span></span>
<span class="line"><span style="color:#E1E4E8">rag_results </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> flockparser.query_remote(</span></span>
<span class="line"><span style="color:#FFAB70">    query</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">user_query,</span></span>
<span class="line"><span style="color:#FFAB70">    embedding</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">embedding,  </span><span style="color:#6A737D"># Skip FlockParser's embedding generation</span></span>
<span class="line"><span style="color:#FFAB70">    n_results</span><span style="color:#F97583">=</span><span style="color:#79B8FF">5</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#6A737D"># FlockParser returns relevant chunks from 41 documents</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Performance gain: 2-5x faster when SOLLOL has faster nodes!</span></span>
<span class="line"></span></code></pre>
<h3 id="production-integrations"><strong>Production Integrations</strong></h3>
<p><strong>SOLLOL is actively used in production by:</strong></p>
<ul>
<li>
<p><strong><a href="https://github.com/BenevolentJoker-JohnL/FlockParser">FlockParser</a></strong> - Document RAG Intelligence with distributed processing. FlockParserâ€™s legacy load balancing code was refactored and became core SOLLOL logic. FlockParser now uses SOLLOL directly via <code>OllamaPool</code> for intelligent routing across document embeddings and LLM queries.</p>
</li>
<li>
<p><strong><a href="https://github.com/BenevolentJoker-JohnL/SynapticLlamas">SynapticLlamas</a></strong> - Multi-agent collaborative research framework. Uses SOLLOLâ€™s <code>HybridRouter</code> for distributed agent execution with RAG-enhanced research capabilities via FlockParser integration.</p>
</li>
</ul>
<p><strong>Related Projects:</strong></p>
<ul>
<li><strong><a href="https://github.com/BenevolentJoker-JohnL/SynapticLlamas">SynapticLlamas</a></strong> - Multi-Agent Orchestration</li>
<li><strong><a href="https://github.com/BenevolentJoker-JohnL/FlockParser">FlockParser</a></strong> - Document RAG Intelligence</li>
</ul>
<hr>
<h3 id="synapticllamas-integration">SynapticLlamas Integration</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#79B8FF"> SOLLOL</span><span style="color:#E1E4E8">, SOLLOLConfig</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> synaptic_llamas </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> AgentOrchestrator</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Setup SOLLOL for multi-agent orchestration</span></span>
<span class="line"><span style="color:#E1E4E8">config </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SOLLOLConfig.auto_discover()</span></span>
<span class="line"><span style="color:#E1E4E8">sollol </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> SOLLOL(config)</span></span>
<span class="line"><span style="color:#E1E4E8">sollol.start(</span><span style="color:#FFAB70">blocking</span><span style="color:#F97583">=</span><span style="color:#79B8FF">False</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># SynapticLlamas now uses SOLLOL for intelligent routing</span></span>
<span class="line"><span style="color:#E1E4E8">orchestrator </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> AgentOrchestrator(</span></span>
<span class="line"><span style="color:#FFAB70">    llm_endpoint</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:8000/api/chat"</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># All agents automatically distributed and optimized</span></span>
<span class="line"><span style="color:#E1E4E8">orchestrator.run_parallel_agents([</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">])</span></span>
<span class="line"></span></code></pre>
<h3 id="flockparser-integration">FlockParser Integration</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># FlockParser uses SOLLOL's OllamaPool directly</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool(</span></span>
<span class="line"><span style="color:#FFAB70">    nodes</span><span style="color:#F97583">=</span><span style="color:#79B8FF">None</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Auto-discover all Ollama nodes</span></span>
<span class="line"><span style="color:#FFAB70">    enable_intelligent_routing</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    exclude_localhost</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    discover_all_nodes</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    app_name</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"FlockParser"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    enable_ray</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># All FlockParser document embeddings and queries route through SOLLOL</span></span>
<span class="line"><span style="color:#E1E4E8">embeddings </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.embed(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"mxbai-embed-large"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"document text"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"query"</span><span style="color:#E1E4E8">}])</span></span>
<span class="line"></span></code></pre>
<h3 id="langchain-integration">LangChain Integration</h3>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> langchain.llms </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> Ollama</span></span>
<span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Use SOLLOL as LangChain backend</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8">llm </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Ollama(</span></span>
<span class="line"><span style="color:#FFAB70">    base_url</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"http://localhost:8000"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># LangChain requests now go through SOLLOL</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> llm(</span><span style="color:#9ECBFF">"What is quantum computing?"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<hr>
<h2 id="-production-deployment-bare-metal">ğŸ­ Production Deployment (Bare Metal)</h2>
<p>For teams preferring bare metal infrastructure over containers, SOLLOL provides systemd-based deployment for production environments.</p>
<h3 id="multi-node-bare-metal-setup"><strong>Multi-Node Bare Metal Setup</strong></h3>
<p>This setup assumes you have 3+ physical machines with Ollama installed. Weâ€™ll configure SOLLOL as a centralized routing layer.</p>
<h4 id="architecture"><strong>Architecture:</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚   Central Router Machine (Control Planeâ”‚</span></span>
<span class="line"><span>â”‚   - SOLLOL Dashboard (port 8080)       â”‚</span></span>
<span class="line"><span>â”‚   - Redis (port 6379)                  â”‚</span></span>
<span class="line"><span>â”‚   - Optional: GPU reporter             â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span>             â”‚ Auto-discovery via network</span></span>
<span class="line"><span>             â”‚ scan (ports 11434)</span></span>
<span class="line"><span>     â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>     â–¼       â–¼          â–¼             â–¼</span></span>
<span class="line"><span>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span></span>
<span class="line"><span>â”‚ Node 1  â”‚ â”‚ Node 2  â”‚ â”‚ Node 3  â”‚ â”‚ Node N  â”‚</span></span>
<span class="line"><span>â”‚ Ollama  â”‚ â”‚ Ollama  â”‚ â”‚ Ollama  â”‚ â”‚ Ollama  â”‚</span></span>
<span class="line"><span>â”‚ :11434  â”‚ â”‚ :11434  â”‚ â”‚ :11434  â”‚ â”‚ :11434  â”‚</span></span>
<span class="line"><span>â”‚ GPU 24GBâ”‚ â”‚ GPU 16GBâ”‚ â”‚ CPU 64c â”‚ â”‚ ...     â”‚</span></span>
<span class="line"><span>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span></span>
<span class="line"><span></span></span></code></pre>
<h4 id="step-1-install-ollama-on-each-node"><strong>Step 1: Install Ollama on each node</strong></h4>
<p>On each worker node (Node 1, 2, 3, â€¦):</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Install Ollama</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#79B8FF"> -fsSL</span><span style="color:#9ECBFF"> https://ollama.ai/install.sh</span><span style="color:#F97583"> |</span><span style="color:#B392F0"> sh</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Start Ollama service</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> enable</span><span style="color:#9ECBFF"> ollama</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> start</span><span style="color:#9ECBFF"> ollama</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Verify it's running</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://localhost:11434/api/tags</span></span>
<span class="line"></span></code></pre>
<h4 id="step-2-install-sollol-on-control-plane-machine"><strong>Step 2: Install SOLLOL on control plane machine</strong></h4>
<p>On your central router machine:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Install SOLLOL and dependencies</span></span>
<span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> sollol</span><span style="color:#9ECBFF"> redis</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Install Redis</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> apt-get</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> redis-server</span><span style="color:#6A737D">  # Ubuntu/Debian</span></span>
<span class="line"><span style="color:#6A737D"># OR</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> yum</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> redis</span><span style="color:#6A737D">              # RHEL/CentOS</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Start Redis</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> enable</span><span style="color:#9ECBFF"> redis</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> start</span><span style="color:#9ECBFF"> redis</span></span>
<span class="line"></span></code></pre>
<h4 id="step-3-create-systemd-service-for-sollol-dashboard"><strong>Step 3: Create systemd service for SOLLOL Dashboard</strong></h4>
<p>Create <code>/etc/systemd/system/sollol-dashboard.service</code>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="ini"><code><span class="line"><span style="color:#B392F0">[Unit]</span></span>
<span class="line"><span style="color:#F97583">Description</span><span style="color:#E1E4E8">=SOLLOL Dashboard Service</span></span>
<span class="line"><span style="color:#F97583">After</span><span style="color:#E1E4E8">=network.target redis.service</span></span>
<span class="line"><span style="color:#F97583">Requires</span><span style="color:#E1E4E8">=redis.service</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">[Service]</span></span>
<span class="line"><span style="color:#F97583">Type</span><span style="color:#E1E4E8">=simple</span></span>
<span class="line"><span style="color:#F97583">User</span><span style="color:#E1E4E8">=sollol  </span><span style="color:#6A737D"># Create dedicated user for security</span></span>
<span class="line"><span style="color:#F97583">Group</span><span style="color:#E1E4E8">=sollol</span></span>
<span class="line"><span style="color:#F97583">WorkingDirectory</span><span style="color:#E1E4E8">=/opt/sollol</span></span>
<span class="line"><span style="color:#F97583">Environment</span><span style="color:#E1E4E8">=</span><span style="color:#9ECBFF">"SOLLOL_DASHBOARD=true"</span></span>
<span class="line"><span style="color:#F97583">Environment</span><span style="color:#E1E4E8">=</span><span style="color:#9ECBFF">"SOLLOL_DASHBOARD_PORT=8080"</span></span>
<span class="line"><span style="color:#F97583">Environment</span><span style="color:#E1E4E8">=</span><span style="color:#9ECBFF">"REDIS_URL=redis://localhost:6379"</span></span>
<span class="line"><span style="color:#F97583">ExecStart</span><span style="color:#E1E4E8">=/usr/bin/python3 -m sollol.dashboard_service --port 8080 --redis-url redis://localhost:6379</span></span>
<span class="line"><span style="color:#F97583">Restart</span><span style="color:#E1E4E8">=always</span></span>
<span class="line"><span style="color:#F97583">RestartSec</span><span style="color:#E1E4E8">=10</span></span>
<span class="line"><span style="color:#F97583">StandardOutput</span><span style="color:#E1E4E8">=journal</span></span>
<span class="line"><span style="color:#F97583">StandardError</span><span style="color:#E1E4E8">=journal</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">[Install]</span></span>
<span class="line"><span style="color:#F97583">WantedBy</span><span style="color:#E1E4E8">=multi-user.target</span></span>
<span class="line"></span></code></pre>
<p>Enable and start:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> useradd</span><span style="color:#79B8FF"> -r</span><span style="color:#79B8FF"> -s</span><span style="color:#9ECBFF"> /bin/</span><span style="color:#79B8FF">false</span><span style="color:#9ECBFF"> sollol</span><span style="color:#6A737D">  # Create dedicated user</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> mkdir</span><span style="color:#79B8FF"> -p</span><span style="color:#9ECBFF"> /opt/sollol</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> chown</span><span style="color:#9ECBFF"> sollol:sollol</span><span style="color:#9ECBFF"> /opt/sollol</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> daemon-reload</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> enable</span><span style="color:#9ECBFF"> sollol-dashboard</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> start</span><span style="color:#9ECBFF"> sollol-dashboard</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Verify</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> status</span><span style="color:#9ECBFF"> sollol-dashboard</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://localhost:8080/health</span></span>
<span class="line"></span></code></pre>
<h4 id="step-4-install-gpu-reporters-on-nodes-optional-but-recommended"><strong>Step 4: Install GPU reporters on nodes (optional but recommended)</strong></h4>
<p>On each GPU node for accurate VRAM monitoring:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Install on each node with GPUs</span></span>
<span class="line"><span style="color:#B392F0">pip</span><span style="color:#9ECBFF"> install</span><span style="color:#9ECBFF"> sollol</span><span style="color:#9ECBFF"> gpustat</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Run GPU reporter (publishes to central Redis)</span></span>
<span class="line"><span style="color:#B392F0">sollol</span><span style="color:#9ECBFF"> install-gpu-reporter</span><span style="color:#79B8FF"> --redis-host</span><span style="color:#F97583"> &#x3C;</span><span style="color:#9ECBFF">control-plane-i</span><span style="color:#E1E4E8">p</span><span style="color:#F97583">></span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Example for node at 10.9.66.45</span></span>
<span class="line"><span style="color:#B392F0">sollol</span><span style="color:#9ECBFF"> install-gpu-reporter</span><span style="color:#79B8FF"> --redis-host</span><span style="color:#79B8FF"> 10.9.66.154</span></span>
<span class="line"></span></code></pre>
<p>Create <code>/etc/systemd/system/sollol-gpu-reporter.service</code> on each GPU node:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="ini"><code><span class="line"><span style="color:#B392F0">[Unit]</span></span>
<span class="line"><span style="color:#F97583">Description</span><span style="color:#E1E4E8">=SOLLOL GPU Reporter</span></span>
<span class="line"><span style="color:#F97583">After</span><span style="color:#E1E4E8">=network.target</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">[Service]</span></span>
<span class="line"><span style="color:#F97583">Type</span><span style="color:#E1E4E8">=simple</span></span>
<span class="line"><span style="color:#F97583">User</span><span style="color:#E1E4E8">=sollol</span></span>
<span class="line"><span style="color:#F97583">ExecStart</span><span style="color:#E1E4E8">=/usr/local/bin/sollol-gpu-reporter --redis-host &#x3C;control-plane-ip> --interval 5</span></span>
<span class="line"><span style="color:#F97583">Restart</span><span style="color:#E1E4E8">=always</span></span>
<span class="line"><span style="color:#F97583">RestartSec</span><span style="color:#E1E4E8">=5</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0">[Install]</span></span>
<span class="line"><span style="color:#F97583">WantedBy</span><span style="color:#E1E4E8">=multi-user.target</span></span>
<span class="line"></span></code></pre>
<h4 id="step-5-configure-firewall-rules"><strong>Step 5: Configure firewall rules</strong></h4>
<p>On all nodes:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Allow Ollama traffic (port 11434)</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> ufw</span><span style="color:#9ECBFF"> allow</span><span style="color:#9ECBFF"> 11434/tcp</span><span style="color:#9ECBFF"> comment</span><span style="color:#9ECBFF"> "Ollama API"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># On control plane only: allow dashboard access</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> ufw</span><span style="color:#9ECBFF"> allow</span><span style="color:#9ECBFF"> 8080/tcp</span><span style="color:#9ECBFF"> comment</span><span style="color:#9ECBFF"> "SOLLOL Dashboard"</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> ufw</span><span style="color:#9ECBFF"> allow</span><span style="color:#9ECBFF"> 6379/tcp</span><span style="color:#9ECBFF"> comment</span><span style="color:#9ECBFF"> "Redis"</span><span style="color:#6A737D">  # Only from trusted nodes</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Reload firewall</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> ufw</span><span style="color:#9ECBFF"> reload</span></span>
<span class="line"></span></code></pre>
<h4 id="step-6-test-the-deployment"><strong>Step 6: Test the deployment</strong></h4>
<p>From any machine with network access:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># SOLLOL auto-discovers all nodes via network scan</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Verify nodes discovered</span></span>
<span class="line"><span style="color:#E1E4E8">stats </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.get_stats()</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Discovered </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'active_nodes'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> nodes"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Make a test request</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Hello!"</span><span style="color:#E1E4E8">}]</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(response[</span><span style="color:#9ECBFF">'message'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'content'</span><span style="color:#E1E4E8">])</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Routed to: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">response[</span><span style="color:#9ECBFF">'_sollol_routing'</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">'host'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h4 id="step-7-monitor-with-systemd"><strong>Step 7: Monitor with systemd</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Check dashboard status</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> status</span><span style="color:#9ECBFF"> sollol-dashboard</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># View live logs</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> journalctl</span><span style="color:#79B8FF"> -u</span><span style="color:#9ECBFF"> sollol-dashboard</span><span style="color:#79B8FF"> -f</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check GPU reporters</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> systemctl</span><span style="color:#9ECBFF"> status</span><span style="color:#9ECBFF"> sollol-gpu-reporter</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># View metrics</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://localhost:8080/api/stats</span><span style="color:#F97583"> |</span><span style="color:#B392F0"> jq</span></span>
<span class="line"></span></code></pre>
<h3 id="production-hardening"><strong>Production Hardening</strong></h3>
<h4 id="security"><strong>Security:</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># 1. Run SOLLOL as dedicated unprivileged user</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> useradd</span><span style="color:#79B8FF"> -r</span><span style="color:#79B8FF"> -s</span><span style="color:#9ECBFF"> /bin/</span><span style="color:#79B8FF">false</span><span style="color:#9ECBFF"> sollol</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 2. Configure Redis authentication</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> vi</span><span style="color:#9ECBFF"> /etc/redis/redis.conf</span></span>
<span class="line"><span style="color:#6A737D"># Add: requirepass &#x3C;strong-password></span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># 3. Use firewall to restrict access</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> ufw</span><span style="color:#9ECBFF"> allow</span><span style="color:#9ECBFF"> from</span><span style="color:#9ECBFF"> 10.9.66.0/24</span><span style="color:#9ECBFF"> to</span><span style="color:#9ECBFF"> any</span><span style="color:#9ECBFF"> port</span><span style="color:#79B8FF"> 6379</span><span style="color:#6A737D">  # Redis from trusted subnet only</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> ufw</span><span style="color:#9ECBFF"> allow</span><span style="color:#9ECBFF"> from</span><span style="color:#9ECBFF"> 10.9.66.0/24</span><span style="color:#9ECBFF"> to</span><span style="color:#9ECBFF"> any</span><span style="color:#9ECBFF"> port</span><span style="color:#79B8FF"> 8080</span><span style="color:#6A737D">  # Dashboard from trusted subnet</span></span>
<span class="line"></span></code></pre>
<h4 id="high-availability"><strong>High Availability:</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Use systemd watchdog for automatic restart on crashes</span></span>
<span class="line"><span style="color:#E1E4E8">[Service]</span></span>
<span class="line"><span style="color:#E1E4E8">WatchdogSec</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">30</span></span>
<span class="line"><span style="color:#E1E4E8">Restart</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">always</span></span>
<span class="line"><span style="color:#E1E4E8">RestartSec</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">10</span></span>
<span class="line"></span></code></pre>
<h4 id="monitoring"><strong>Monitoring:</strong></h4>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Integrate with Prometheus</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://localhost:9090/metrics</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Or use systemd monitoring</span></span>
<span class="line"><span style="color:#B392F0">systemctl</span><span style="color:#9ECBFF"> status</span><span style="color:#9ECBFF"> sollol-dashboard</span><span style="color:#F97583"> |</span><span style="color:#B392F0"> grep</span><span style="color:#9ECBFF"> "Active:"</span></span>
<span class="line"></span></code></pre>
<h3 id="troubleshooting"><strong>Troubleshooting</strong></h3>
<p><strong>Nodes not discovered:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Check network connectivity</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> ip </span><span style="color:#F97583">in</span><span style="color:#9ECBFF"> 10.9.66.</span><span style="color:#E1E4E8">{</span><span style="color:#B392F0">1..255}</span><span style="color:#E1E4E8">; </span><span style="color:#F97583">do</span></span>
<span class="line"><span style="color:#B392F0">    timeout</span><span style="color:#79B8FF"> 0.5</span><span style="color:#9ECBFF"> bash</span><span style="color:#79B8FF"> -c</span><span style="color:#9ECBFF"> "cat &#x3C; /dev/null > /dev/tcp/</span><span style="color:#E1E4E8">$ip</span><span style="color:#9ECBFF">/11434 2>/dev/null"</span><span style="color:#E1E4E8"> &#x26;&#x26; </span><span style="color:#79B8FF">echo</span><span style="color:#9ECBFF"> "</span><span style="color:#E1E4E8">$ip</span><span style="color:#9ECBFF">:11434 reachable"</span></span>
<span class="line"><span style="color:#F97583">done</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check Ollama is listening on all interfaces (not just localhost)</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://</span><span style="color:#F97583">&#x3C;</span><span style="color:#9ECBFF">node-i</span><span style="color:#E1E4E8">p</span><span style="color:#F97583">></span><span style="color:#9ECBFF">:11434/api/tags</span></span>
<span class="line"></span></code></pre>
<p><strong>Dashboard not starting:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Check Redis is running</span></span>
<span class="line"><span style="color:#B392F0">systemctl</span><span style="color:#9ECBFF"> status</span><span style="color:#9ECBFF"> redis</span></span>
<span class="line"><span style="color:#B392F0">redis-cli</span><span style="color:#9ECBFF"> ping</span><span style="color:#6A737D">  # Should return "PONG"</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check port not in use</span></span>
<span class="line"><span style="color:#B392F0">sudo</span><span style="color:#9ECBFF"> lsof</span><span style="color:#79B8FF"> -i</span><span style="color:#9ECBFF"> :8080</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># View detailed logs</span></span>
<span class="line"><span style="color:#B392F0">journalctl</span><span style="color:#79B8FF"> -u</span><span style="color:#9ECBFF"> sollol-dashboard</span><span style="color:#79B8FF"> --since</span><span style="color:#9ECBFF"> "10 minutes ago"</span></span>
<span class="line"></span></code></pre>
<p><strong>Performance issues:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Check node health</span></span>
<span class="line"><span style="color:#B392F0">curl</span><span style="color:#9ECBFF"> http://localhost:8080/api/stats</span><span style="color:#F97583"> |</span><span style="color:#B392F0"> jq</span><span style="color:#9ECBFF"> '.node_performance'</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Monitor resource usage</span></span>
<span class="line"><span style="color:#B392F0">htop</span></span>
<span class="line"><span style="color:#B392F0">nvidia-smi</span><span style="color:#6A737D">  # On GPU nodes</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check network latency between nodes</span></span>
<span class="line"><span style="color:#B392F0">ping</span><span style="color:#F97583"> &#x3C;</span><span style="color:#9ECBFF">node-i</span><span style="color:#E1E4E8">p</span><span style="color:#F97583">></span></span>
<span class="line"></span></code></pre>
<hr>
<h2 id="-documentation">ğŸ“š Documentation</h2>
<ul>
<li><strong><a href="ARCHITECTURE.md">Architecture Guide</a></strong> - Deep dive into system design</li>
<li><strong><a href="BACKENDS.md">Backend Architecture</a></strong> - Backend extensibility and adding new LLM backends</li>
<li><strong><a href="BATCH_API.md">Batch Processing API</a></strong> - Complete guide to batch job management (NEW in v0.7.0)
<ul>
<li>API endpoints and examples</li>
<li>Job lifecycle and progress tracking</li>
<li>Best practices and error handling</li>
</ul>
</li>
<li><strong><a href="docs/llama_cpp_guide.md">llama.cpp Distributed Inference Guide</a></strong> - Complete guide to model sharding
<ul>
<li>Setup and configuration</li>
<li>Performance optimization</li>
<li>Troubleshooting common issues</li>
<li>Advanced topics (custom layer distribution, monitoring, etc.)</li>
</ul>
</li>
<li><strong><a href="examples/integration/">Integration Examples</a></strong> - Practical integration patterns
<ul>
<li><a href="examples/integration/sync_agents.py">Synchronous Agent Integration</a></li>
<li><a href="examples/integration/priority_mapping.py">Priority Configuration</a></li>
<li><a href="examples/integration/load_balancer_wrapper.py">Load Balancer Wrapper</a></li>
</ul>
</li>
<li><strong><a href="examples/llama_cpp_distributed.py">llama.cpp Distributed Examples</a></strong> - Model sharding examples
<ul>
<li>Auto-setup and manual configuration</li>
<li>Multi-turn conversations with monitoring</li>
<li>Batch processing with multiple models</li>
<li>Error handling and recovery patterns</li>
</ul>
</li>
<li><strong><a href="docs/deployment.md">Deployment Guide</a></strong> - Production deployment patterns</li>
<li><strong><a href="docs/api.md">API Reference</a></strong> - Complete API documentation</li>
<li><strong><a href="docs/performance.md">Performance Tuning</a></strong> - Optimization guide</li>
<li><strong><a href="SYNAPTICLLAMAS_LEARNINGS.md">SynapticLlamas Learnings</a></strong> - Features from production use</li>
</ul>
<hr>
<h2 id="-whats-new-in-v070">ğŸ†• Whatâ€™s New in v0.7.0</h2>
<h3 id="-batch-processing-api">ğŸ“¦ Batch Processing API</h3>
<p>Complete RESTful API for asynchronous batch job management. Submit large-scale batch operations (embeddings, bulk inference) and track progress via job IDs.</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> requests</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Submit batch embedding job (up to 10,000 documents)</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.post(</span><span style="color:#9ECBFF">"http://localhost:11434/api/batch/embed"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">json</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">{</span></span>
<span class="line"><span style="color:#9ECBFF">    "model"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"nomic-embed-text"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#9ECBFF">    "documents"</span><span style="color:#E1E4E8">: [</span><span style="color:#9ECBFF">"doc1"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"doc2"</span><span style="color:#E1E4E8">, </span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">],  </span><span style="color:#6A737D"># Thousands of documents</span></span>
<span class="line"><span style="color:#E1E4E8">})</span></span>
<span class="line"><span style="color:#E1E4E8">job_id </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> response.json()[</span><span style="color:#9ECBFF">"job_id"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check status</span></span>
<span class="line"><span style="color:#E1E4E8">status </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.get(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"http://localhost:11434/api/batch/jobs/</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">job_id</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(status.json()[</span><span style="color:#9ECBFF">"progress"</span><span style="color:#E1E4E8">][</span><span style="color:#9ECBFF">"percent"</span><span style="color:#E1E4E8">])  </span><span style="color:#6A737D"># 100.0</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Get results</span></span>
<span class="line"><span style="color:#E1E4E8">results </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> requests.get(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"http://localhost:11434/api/batch/results/</span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">job_id</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">embeddings </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> results.json()[</span><span style="color:#9ECBFF">"results"</span><span style="color:#E1E4E8">]</span></span>
<span class="line"></span></code></pre>
<p><strong>Batch API Endpoints:</strong></p>
<ul>
<li><code>POST /api/batch/embed</code> - Submit batch embedding job</li>
<li><code>GET /api/batch/jobs/{job_id}</code> - Get job status with progress tracking</li>
<li><code>GET /api/batch/results/{job_id}</code> - Retrieve job results and errors</li>
<li><code>DELETE /api/batch/jobs/{job_id}</code> - Cancel running jobs</li>
<li><code>GET /api/batch/jobs?limit=100</code> - List recent jobs</li>
</ul>
<p><strong>Features:</strong></p>
<ul>
<li>UUID-based job tracking with 5 states (PENDING, RUNNING, COMPLETED, FAILED, CANCELLED)</li>
<li>Automatic TTL-based cleanup (1 hour default)</li>
<li>Progress tracking: completed_items, failed_items, percentage</li>
<li>Duration calculation and metadata storage</li>
<li>Async job execution via Dask distributed processing</li>
</ul>
<hr>
<h2 id="-performance-optimizations-v0918">âš¡ Performance Optimizations (v0.9.18+)</h2>
<p>SOLLOL now includes <strong>8 production-grade performance optimizations</strong> designed to improve throughput and latency:</p>
<p><strong>âš ï¸ Transparency Note:</strong> These features are implemented and functional, but claimed performance improvements are projections based on architecture, NOT independently validated benchmarks. See <a href="#-performance-impact">Performance Impact</a> section below for details.</p>
<h3 id="-response-caching-layer">ğŸš€ Response Caching Layer</h3>
<p><strong>Expected Impact:</strong> Reduces latency for repeated queries (cache hit/miss tracking validated)</p>
<p>Intelligent LRU cache with TTL expiration:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Enable response caching (enabled by default)</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure(</span></span>
<span class="line"><span style="color:#FFAB70">    enable_cache</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    cache_max_size</span><span style="color:#F97583">=</span><span style="color:#79B8FF">1000</span><span style="color:#E1E4E8">,  </span><span style="color:#6A737D"># Cache up to 1000 responses</span></span>
<span class="line"><span style="color:#FFAB70">    cache_ttl</span><span style="color:#F97583">=</span><span style="color:#79B8FF">3600</span><span style="color:#6A737D">        # 1 hour TTL</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># First request: normal latency</span></span>
<span class="line"><span style="color:#E1E4E8">response1 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.embed(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"mxbai-embed-large"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"Hello world"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Cached request: faster</span></span>
<span class="line"><span style="color:#E1E4E8">response2 </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.embed(</span><span style="color:#FFAB70">model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"mxbai-embed-large"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"Hello world"</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># Cache hit</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Programmatic cache management</span></span>
<span class="line"><span style="color:#E1E4E8">pool.clear_cache()                              </span><span style="color:#6A737D"># Clear all</span></span>
<span class="line"><span style="color:#E1E4E8">pool.invalidate_cache_by_model(</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">)     </span><span style="color:#6A737D"># Invalidate by model</span></span>
<span class="line"><span style="color:#E1E4E8">cache_data </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.export_cache()                </span><span style="color:#6A737D"># Export for persistence</span></span>
<span class="line"><span style="color:#E1E4E8">pool.import_cache(cache_data)                   </span><span style="color:#6A737D"># Restore from export</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Get cache stats</span></span>
<span class="line"><span style="color:#E1E4E8">stats </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.get_cache_stats()</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Hit rate: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'hit_rate'</span><span style="color:#E1E4E8">]</span><span style="color:#F97583">:.1%</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)    </span><span style="color:#6A737D"># 85.2%</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Cache size: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'size'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)           </span><span style="color:#6A737D"># 234/1000</span></span>
<span class="line"></span></code></pre>
<h3 id="-streaming-support">ğŸŒŠ Streaming Support</h3>
<p><strong>Expected Impact:</strong> Better UX, reduced perceived latency (streaming functionality validated)</p>
<p>Token-by-token streaming for <code>chat()</code> and <code>generate()</code>:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Stream chat responses</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> chunk </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> pool.chat(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[{</span><span style="color:#9ECBFF">"role"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"user"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">: </span><span style="color:#9ECBFF">"Tell me a story"</span><span style="color:#E1E4E8">}],</span></span>
<span class="line"><span style="color:#FFAB70">    stream</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span></span>
<span class="line"><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#E1E4E8">    content </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> chunk.get(</span><span style="color:#9ECBFF">"message"</span><span style="color:#E1E4E8">, {}).get(</span><span style="color:#9ECBFF">"content"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(content, </span><span style="color:#FFAB70">end</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">flush</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Stream text generation</span></span>
<span class="line"><span style="color:#F97583">for</span><span style="color:#E1E4E8"> chunk </span><span style="color:#F97583">in</span><span style="color:#E1E4E8"> pool.generate(</span></span>
<span class="line"><span style="color:#FFAB70">    model</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    prompt</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"Explain quantum computing"</span><span style="color:#E1E4E8">,</span></span>
<span class="line"><span style="color:#FFAB70">    stream</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span></span>
<span class="line"><span style="color:#E1E4E8">):</span></span>
<span class="line"><span style="color:#79B8FF">    print</span><span style="color:#E1E4E8">(chunk.get(</span><span style="color:#9ECBFF">"response"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">), </span><span style="color:#FFAB70">end</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">""</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">flush</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h3 id="-smart-model-prefetching">ğŸ”¥ Smart Model Prefetching</h3>
<p><strong>Expected Impact:</strong> 1-5 seconds reduced first-request latency (projection, not measured)</p>
<p>Pre-load models into VRAM before first use:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Warm a single model</span></span>
<span class="line"><span style="color:#E1E4E8">pool.warm_model(</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Warm multiple models in parallel</span></span>
<span class="line"><span style="color:#E1E4E8">results </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.warm_models(</span></span>
<span class="line"><span style="color:#FFAB70">    models</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"codellama"</span><span style="color:#E1E4E8">, </span><span style="color:#9ECBFF">"mistral"</span><span style="color:#E1E4E8">],</span></span>
<span class="line"><span style="color:#FFAB70">    parallel</span><span style="color:#F97583">=</span><span style="color:#79B8FF">True</span></span>
<span class="line"><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"Warmed </span><span style="color:#79B8FF">{sum</span><span style="color:#E1E4E8">(results.values())</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF"> models"</span><span style="color:#E1E4E8">)</span></span>
<span class="line"></span></code></pre>
<h3 id="-async-io-support">âš¡ Async I/O Support</h3>
<p><strong>Expected Impact:</strong> 2-3x throughput for concurrent requests (projection, not measured)</p>
<p>True non-blocking I/O with httpx AsyncClient:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">import</span><span style="color:#E1E4E8"> asyncio</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Async methods for concurrent requests</span></span>
<span class="line"><span style="color:#F97583">async</span><span style="color:#F97583"> def</span><span style="color:#B392F0"> process_batch</span><span style="color:#E1E4E8">():</span></span>
<span class="line"><span style="color:#E1E4E8">    responses </span><span style="color:#F97583">=</span><span style="color:#F97583"> await</span><span style="color:#E1E4E8"> asyncio.gather(</span></span>
<span class="line"><span style="color:#E1E4E8">        pool.chat_async(</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">messages</span><span style="color:#F97583">=</span><span style="color:#E1E4E8">[</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">]),</span></span>
<span class="line"><span style="color:#E1E4E8">        pool.generate_async(</span><span style="color:#9ECBFF">"llama3.2"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">prompt</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"..."</span><span style="color:#E1E4E8">),</span></span>
<span class="line"><span style="color:#E1E4E8">        pool.embed_async(</span><span style="color:#9ECBFF">"mxbai-embed-large"</span><span style="color:#E1E4E8">, </span><span style="color:#FFAB70">input</span><span style="color:#F97583">=</span><span style="color:#9ECBFF">"..."</span><span style="color:#E1E4E8">)</span></span>
<span class="line"><span style="color:#E1E4E8">    )</span></span>
<span class="line"><span style="color:#F97583">    return</span><span style="color:#E1E4E8"> responses</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Run async batch</span></span>
<span class="line"><span style="color:#E1E4E8">results </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> asyncio.run(process_batch())</span></span>
<span class="line"></span></code></pre>
<h3 id="-http2-multiplexing">ğŸ”— HTTP/2 Multiplexing</h3>
<p><strong>Expected Impact:</strong> 30-50% latency reduction for concurrent requests (projection, not measured)</p>
<p>Automatic HTTP/2 support when <code>httpx</code> is installed:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#6A737D"># Automatically uses HTTP/2 if available</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D"># Check if HTTP/2 is enabled</span></span>
<span class="line"><span style="color:#E1E4E8">stats </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.get_stats()</span></span>
<span class="line"><span style="color:#79B8FF">print</span><span style="color:#E1E4E8">(</span><span style="color:#F97583">f</span><span style="color:#9ECBFF">"HTTP/2 enabled: </span><span style="color:#79B8FF">{</span><span style="color:#E1E4E8">stats[</span><span style="color:#9ECBFF">'http2_enabled'</span><span style="color:#E1E4E8">]</span><span style="color:#79B8FF">}</span><span style="color:#9ECBFF">"</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># True</span></span>
<span class="line"></span></code></pre>
<h3 id="-additional-optimizations">ğŸ“Š Additional Optimizations</h3>
<p><strong>Connection Pool Tuning</strong> (10-20% better concurrency):</p>
<ul>
<li>Optimized pool sizes: 10-20 connections per node</li>
<li>Automatic retry with exponential backoff</li>
<li>Connection reuse with keep-alive</li>
</ul>
<p><strong>Adaptive Health Checks</strong> (5-10% overhead reduction):</p>
<ul>
<li>Dynamic intervals based on node stability:
<ul>
<li>Very stable (&#x3C;1% failures): 60s interval</li>
<li>Stable (&#x3C;5% failures): 30s interval</li>
<li>Degraded (5-15% failures): 15s interval</li>
<li>Unstable (>15% failures): 5s interval</li>
</ul>
</li>
</ul>
<p><strong>Telemetry Sampling</strong> (~90% overhead reduction):</p>
<ul>
<li>Configurable sampling for info-level events (default: 10%)</li>
<li>Always logs errors and critical events</li>
<li>Reduces dashboard logging overhead</li>
</ul>
<h3 id="-performance-impact">ğŸ“ˆ Performance Impact</h3>
<p><strong>âš ï¸ IMPORTANT: These are architectural projections, NOT measured results</strong></p>
<p>These optimizations are implemented and functional, but multi-node performance gains have not been independently validated:</p>
<p><strong>Projected improvements (unvalidated):</strong></p>
<ul>
<li><strong>Throughput:</strong> +150-300% for concurrent workloads (theory: parallel request handling)</li>
<li><strong>Latency:</strong> -40-70% for typical requests (theory: caching + HTTP/2)</li>
<li><strong>Cache hits:</strong> Significant latency reduction for repeated queries (validated in single-node tests)</li>
</ul>
<p><strong>Whatâ€™s actually measured:</strong></p>
<ul>
<li>âœ… Response caching works (cache hit/miss rates tracked)</li>
<li>âœ… Streaming works (token-by-token delivery confirmed)</li>
<li>âœ… HTTP/2 enabled (httpx connection verified)</li>
<li>âš ï¸ Multi-node throughput gains: Not independently benchmarked</li>
</ul>
<p><strong>To validate these claims yourself:</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="bash"><code><span class="line"><span style="color:#6A737D"># Run comparative benchmarks</span></span>
<span class="line"><span style="color:#79B8FF">cd</span><span style="color:#9ECBFF"> benchmarks</span></span>
<span class="line"><span style="color:#B392F0">python</span><span style="color:#9ECBFF"> run_benchmarks.py</span><span style="color:#79B8FF"> --sollol-url</span><span style="color:#9ECBFF"> http://localhost:8000</span><span style="color:#79B8FF"> --duration</span><span style="color:#79B8FF"> 120</span></span>
<span class="line"></span></code></pre>
<p>See <a href="BENCHMARKING.md">BENCHMARKING.md</a> for methodology.</p>
<hr>
<h3 id="previous-features-v036">Previous Features (v0.3.6+)</h3>
<p><strong>Synchronous API</strong> - No async/await required:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.sync_wrapper </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> OllamaPool</span></span>
<span class="line"><span style="color:#E1E4E8">pool </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> OllamaPool.auto_configure()</span></span>
<span class="line"><span style="color:#E1E4E8">response </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> pool.chat(</span><span style="color:#79B8FF">...</span><span style="color:#E1E4E8">)  </span><span style="color:#6A737D"># Synchronous call</span></span>
<span class="line"></span></code></pre>
<p><strong>Priority Helpers</strong> - Semantic priority levels:</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#F97583">from</span><span style="color:#E1E4E8"> sollol.priority_helpers </span><span style="color:#F97583">import</span><span style="color:#E1E4E8"> Priority</span></span>
<span class="line"><span style="color:#E1E4E8">priority </span><span style="color:#F97583">=</span><span style="color:#E1E4E8"> Priority.</span><span style="color:#79B8FF">HIGH</span><span style="color:#6A737D">  # 7</span></span>
<span class="line"></span></code></pre>
<p><strong>SOLLOL Detection:</strong></p>
<ul>
<li><code>X-Powered-By: SOLLOL</code> header on all responses</li>
<li><code>/api/health</code> endpoint returns <code>{"service": "SOLLOL", "version": "0.7.0"}</code></li>
</ul>
<hr>
<h2 id="-comparison">ğŸ†š Comparison</h2>
<h3 id="sollol-vs-simple-load-balancers">SOLLOL vs. Simple Load Balancers</h3>













































<table><thead><tr><th>Feature</th><th>nginx/HAProxy</th><th>SOLLOL</th></tr></thead><tbody><tr><td>Routing</td><td>Round-robin/random</td><td>Context-aware, adapts from history</td></tr><tr><td>Resource awareness</td><td>None</td><td>GPU/CPU/memory-aware</td></tr><tr><td>Failover</td><td>Manual config</td><td>Automatic detection &#x26; recovery</td></tr><tr><td>Model sharding</td><td>âŒ</td><td>âœ… llama.cpp integration</td></tr><tr><td>Task prioritization</td><td>âŒ</td><td>âœ… Priority queue</td></tr><tr><td>Observability</td><td>Basic</td><td>Rich metrics + dashboard</td></tr><tr><td>Setup</td><td>Complex config</td><td>Auto-discover</td></tr></tbody></table>
<h3 id="sollol-vs-kubernetes">SOLLOL vs. Kubernetes</h3>



































<table><thead><tr><th>Feature</th><th>Kubernetes</th><th>SOLLOL</th></tr></thead><tbody><tr><td><strong>Complexity</strong></td><td>High - requires cluster setup</td><td>Low - pip install</td></tr><tr><td><strong>AI-specific</strong></td><td>Generic container orchestration</td><td>Purpose-built for LLMs</td></tr><tr><td><strong>Intelligence</strong></td><td>None</td><td>Task-aware routing</td></tr><tr><td><strong>Model sharding</strong></td><td>Manual</td><td>Automatic</td></tr><tr><td><strong>Best for</strong></td><td>Large-scale production</td><td>AI-focused teams</td></tr></tbody></table>
<p><strong>Use both!</strong> Deploy SOLLOL on Kubernetes for ultimate scalability.</p>
<hr>
<h2 id="-contributing">ğŸ¤ Contributing</h2>
<p>We welcome contributions! Areas weâ€™d love help with:</p>
<ul>
<li>ML-based routing predictions</li>
<li>Additional monitoring integrations</li>
<li>Cloud provider integrations</li>
<li>Performance optimizations</li>
<li>Documentation improvements</li>
</ul>
<p>See <a href="CONTRIBUTING.md">CONTRIBUTING.md</a> for guidelines.</p>
<hr>
<h2 id="-license">ğŸ“œ License</h2>
<p>MIT License - see <a href="LICENSE">LICENSE</a> file for details.</p>
<hr>
<h2 id="-credits">ğŸ™ Credits</h2>
<p>Created by <a href="https://github.com/BenevolentJoker-JohnL">BenevolentJoker-JohnL</a></p>
<p><strong>Part of the Complete AI Ecosystem:</strong></p>
<ul>
<li><strong><a href="https://github.com/BenevolentJoker-JohnL/SynapticLlamas">SynapticLlamas</a></strong> - Multi-Agent Orchestration</li>
<li><strong><a href="https://github.com/BenevolentJoker-JohnL/FlockParser">FlockParser</a></strong> - Document RAG Intelligence</li>
<li><strong><a href="https://github.com/BenevolentJoker-JohnL/SOLLOL">SOLLOL</a></strong> - Distributed Inference Platform (this project)</li>
</ul>
<p><strong>Special Thanks:</strong></p>
<ul>
<li><strong><a href="https://github.com/DallanL">Dallan Loomis</a></strong> - For always providing invaluable support, feedback, and guidance throughout development. Your insights and encouragement have been instrumental in shaping this project.</li>
</ul>
<p>Built with: Ray, Dask, FastAPI, llama.cpp, Ollama</p>
<hr>
<h2 id="-what-makes-sollol-different">ğŸ¯ What Makes SOLLOL Different?</h2>
<ol>
<li><strong>Combines task distribution AND model sharding</strong> in one system</li>
<li><strong>Context-aware routing</strong> that adapts based on performance metrics</li>
<li><strong>Auto-discovery</strong> of nodes with minimal configuration</li>
<li><strong>Built-in failover</strong> and priority queuing</li>
<li><strong>Purpose-built for Ollama clusters</strong> (understands GPU requirements, task types)</li>
</ol>
<p><strong>Limitations to know</strong>:</p>
<ul>
<li>Model sharding verified with 13B models; larger models not extensively tested</li>
<li>Performance benefits depend on network latency and workload patterns</li>
<li>Not a drop-in replacement for single-node setups in all scenarios</li>
</ul>
<hr>
<div align="center">
<p><strong>Stop manually managing your LLM cluster. Let SOLLOL optimize it for you.</strong></p>
<p><a href="#quick-start">Get Started</a> â€¢ <a href="https://github.com/BenevolentJoker-JohnL/SOLLOL">View on GitHub</a> â€¢ <a href="https://github.com/BenevolentJoker-JohnL/SOLLOL/issues">Report Issue</a></p>
</div></div> </div> </section> </article>  </main> <footer class="relative border-t border-slate-800/80 bg-surface/80 py-12 text-sm text-slate-400 backdrop-blur"> <div class="absolute inset-x-0 top-0 h-px bg-gradient-to-r from-transparent via-accent/60 to-transparent" aria-hidden="true"></div> <div class="mx-auto flex max-w-5xl flex-col gap-10 px-6 md:flex-row md:justify-between"> <div> <p class="font-mono text-xs uppercase tracking-[0.3em] text-accent-light">$ tail -f now</p> <h2 class="mt-2 text-lg font-semibold text-white">Letâ€™s connect</h2> <div class="mt-5 grid gap-3 sm:grid-cols-2"> <button type="button" class="group flex items-center justify-between gap-3 rounded-2xl border border-slate-700/60 bg-slate-950/40 px-4 py-3 text-left font-mono text-xs uppercase tracking-[0.35em] text-slate-300 transition hover:border-accent/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-accent" data-copy="alex.schott2002@gmail.com"> <span class="flex items-center gap-3 text-[0.7rem] tracking-[0.4em]"> <span class="flex h-9 w-9 items-center justify-center rounded-full border border-accent/50 bg-accent/10 text-accent-light shadow-inner shadow-cyan-400/20"> <svg class="h-4 w-4 fill-current" viewBox="0 0 24 24" aria-hidden="true"> <path d="M4 6a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v12a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6zm2 .5 6 4 6-4V6H6v.5z"></path> </svg> </span> <span class="flex flex-col gap-0.5 text-left text-white"> <span class="copy-default text-sm font-semibold uppercase tracking-[0.35em]">Email</span> <span class="copy-success hidden text-sm font-semibold uppercase tracking-[0.35em] text-accent-light">Copied!</span> <span class="text-[0.6rem] normal-case tracking-normal text-slate-400">Copy the address for direct outreach.</span> </span> </span> </button><a class="group flex items-center justify-between gap-3 rounded-2xl border border-slate-700/60 bg-slate-950/40 px-4 py-3 text-left font-mono text-xs uppercase tracking-[0.35em] text-slate-300 transition hover:border-accent/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-accent" href="https://github.com/hackall360" target="_blank" rel="noopener noreferrer"> <span class="flex items-center gap-3 text-[0.7rem] tracking-[0.4em]"> <span class="flex h-9 w-9 items-center justify-center rounded-full border border-accent/50 bg-accent/10 text-accent-light shadow-inner shadow-cyan-400/20"> <svg class="h-4 w-4 fill-current" viewBox="0 0 24 24" aria-hidden="true"> <path d="M12 .5a10 10 0 0 0-3.2 19.5c.5.1.7-.2.7-.5v-1.7c-3 0-3.7-1.4-3.9-2.7-.1-.4-.6-1-1-1.2-.3-.2-.8-.6 0-.6.7.1 1.2.7 1.4 1 .8 1.3 2.1.9 2.6.7.1-.6.3-1 .6-1.2-2.7-.3-5.6-1.4-5.6-6a4.6 4.6 0 0 1 1.2-3.2 4.2 4.2 0 0 1 .1-3.1s1-.3 3.3 1.2a11.4 11.4 0 0 1 6 0c2.3-1.5 3.3-1.2 3.3-1.2a4.2 4.2 0 0 1 .1 3.1 4.6 4.6 0 0 1 1.2 3.2c0 4.6-2.9 5.7-5.6 6 .4.3.7.9.7 1.8v2.6c0 .3.2.6.7.5A10 10 0 0 0 12 .5z"></path> </svg> </span> <span class="flex flex-col gap-0.5 text-left text-white"> <span class="text-sm font-semibold uppercase tracking-[0.35em]">GitHub</span> <span class="text-[0.6rem] normal-case tracking-normal text-slate-400">Open the latest experiments in a new tab.</span> </span> </span> <svg class="h-4 w-4 text-accent-light transition group-hover:translate-x-1" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="1.5" aria-hidden="true"> <path d="M7.5 5H15v7.5" stroke-linecap="round" stroke-linejoin="round"></path> <path d="M15 5 5 15" stroke-linecap="round"></path> </svg> </a><a class="group flex items-center justify-between gap-3 rounded-2xl border border-slate-700/60 bg-slate-950/40 px-4 py-3 text-left font-mono text-xs uppercase tracking-[0.35em] text-slate-300 transition hover:border-accent/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-accent" href="https://www.linkedin.com/in/alex-schott-395628233/" target="_blank" rel="noopener noreferrer"> <span class="flex items-center gap-3 text-[0.7rem] tracking-[0.4em]"> <span class="flex h-9 w-9 items-center justify-center rounded-full border border-accent/50 bg-accent/10 text-accent-light shadow-inner shadow-cyan-400/20"> <svg class="h-4 w-4 fill-current" viewBox="0 0 24 24" aria-hidden="true"> <path d="M5 3a2 2 0 1 1 0 4 2 2 0 0 1 0-4zm-2 6h4v12H3V9zm6 0h3.6v1.8h.1c.5-.9 1.8-1.8 3.6-1.8 3.9 0 4.6 2.4 4.6 5.4V21h-4v-5.2c0-1.2 0-2.8-1.8-2.8s-2 1.4-2 2.7V21H9V9z"></path> </svg> </span> <span class="flex flex-col gap-0.5 text-left text-white"> <span class="text-sm font-semibold uppercase tracking-[0.35em]">LinkedIn</span> <span class="text-[0.6rem] normal-case tracking-normal text-slate-400">Connect on LinkedIn in a new tab.</span> </span> </span> <svg class="h-4 w-4 text-accent-light transition group-hover:translate-x-1" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="1.5" aria-hidden="true"> <path d="M7.5 5H15v7.5" stroke-linecap="round" stroke-linejoin="round"></path> <path d="M15 5 5 15" stroke-linecap="round"></path> </svg> </a><a class="group flex items-center justify-between gap-3 rounded-2xl border border-slate-700/60 bg-slate-950/40 px-4 py-3 text-left font-mono text-xs uppercase tracking-[0.35em] text-slate-300 transition hover:border-accent/70 focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-accent" href="https://profile.indeed.com/p/alexs-hxc5xkz" target="_blank" rel="noopener noreferrer"> <span class="flex items-center gap-3 text-[0.7rem] tracking-[0.4em]"> <span class="flex h-9 w-9 items-center justify-center rounded-full border border-accent/50 bg-accent/10 text-accent-light shadow-inner shadow-cyan-400/20"> <svg class="h-4 w-4 fill-current" viewBox="0 0 24 24" aria-hidden="true"> <path d="M12 2a10 10 0 1 0 10 10A10 10 0 0 0 12 2zm0 3.5a2 2 0 1 1-2 2 2 2 0 0 1 2-2zm3.8 11.9H8.2a1 1 0 0 1-.95-1.32l2.1-6.3a1 1 0 0 1 1.9.02l1.2 3.7.88-2.2a1 1 0 0 1 1.87-.05l2.1 4.9a1 1 0 0 1-.92 1.35z"></path> </svg> </span> <span class="flex flex-col gap-0.5 text-left text-white"> <span class="text-sm font-semibold uppercase tracking-[0.35em]">Indeed</span> <span class="text-[0.6rem] normal-case tracking-normal text-slate-400">View experience highlights on Indeed.</span> </span> </span> <svg class="h-4 w-4 text-accent-light transition group-hover:translate-x-1" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="1.5" aria-hidden="true"> <path d="M7.5 5H15v7.5" stroke-linecap="round" stroke-linejoin="round"></path> <path d="M15 5 5 15" stroke-linecap="round"></path> </svg> </a> </div> </div> <div class="grid gap-4 sm:grid-cols-2"> <div class="rounded-xl border border-slate-800/80 bg-surface-elevated/80 p-5 text-slate-200 shadow-[0_0_30px_-15px_rgba(45,212,191,0.6)]"> <p class="font-mono text-xs uppercase tracking-widest text-accent-light">Shipments</p> <p class="mt-2 text-3xl font-semibold text-white">42+</p> <p class="mt-1 text-xs text-slate-400">Production launches & experiments.</p> </div> <div class="rounded-xl border border-slate-800/80 bg-surface-elevated/80 p-5 text-slate-200 shadow-[0_0_30px_-15px_rgba(14,165,233,0.6)]"> <p class="font-mono text-xs uppercase tracking-widest text-accent-light">Uptime</p> <p class="mt-2 text-3xl font-semibold text-white">99.9%</p> <p class="mt-1 text-xs text-slate-400">Projects maintained with obsessive care.</p> </div> </div> </div> <div class="mx-auto mt-10 max-w-5xl px-6 text-xs text-slate-500"> <p>Built with Astro & Tailwind. Deploying curiosity daily.</p> <p class="mt-2">
Analytics respect Do Not Track. Set <code class="rounded bg-slate-800 px-1.5 py-0.5 text-[0.6rem] uppercase tracking-[0.3em] text-accent-light">localStorage.plausible_ignore = "true"</code>
to opt out instantly.
</p> </div> </footer> </div> <script type="module" src="/_astro/client.d839bed6.js"></script> </body> </html>